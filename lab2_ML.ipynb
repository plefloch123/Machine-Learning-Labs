{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBmOdXoenFw"
      },
      "source": [
        "# Lab 2: K-Nearest Neighbours\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iS23ADfo4w"
      },
      "source": [
        "## Version history\n",
        "\n",
        "| Date | Author | Description |\n",
        "|:----:|:------:|:------------|\n",
        "2021-01-18 | Josiah Wang | First version |\n",
        "2021-10-18 | Josiah Wang | Fixed typo in Distance Weighted K-NN Classifier evaluation. The last piece of code should say WeightedKNNClassifier(k) and not KNNClassifier(k), obviously! |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EbbkgqOgZK_"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab exercise is to give you some practical experience to build a K-Nearest Neighbours (K-NN) classifier.\n",
        "\n",
        "By the end of this lab exercise, you will have constructed different variants of a K-NN classifier as discussed in the lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1PH_TJZhf-c"
      },
      "source": [
        "## The Iris dataset\n",
        "\n",
        "Continuing where we left off in Lab 1, we will again work with the Iris dataset.\n",
        "\n",
        "I will just copy and rerun my solutions from Lab 1. Feel free to copy your own implementations over."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdL1OHXvifcA",
        "outputId": "007136a5-96cf-47fc-8730-6fef1983453e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "\n",
        "# Download iris data if it does not exist\n",
        "if not os.path.exists(\"iris.data\"):\n",
        "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "\n",
        "\n",
        "def read_dataset(filepath):\n",
        "    \"\"\" Read in the dataset from the specified filepath\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The filepath to the dataset file\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x, y, classes), each being a numpy array.\n",
        "               - x is a numpy array with shape (N, K),\n",
        "                   where N is the number of instances\n",
        "                   K is the number of features/attributes\n",
        "               - y is a numpy array with shape (N, ), and should be integers from 0 to C-1\n",
        "                   where C is the number of classes\n",
        "               - classes : a numpy array with shape (C, ), which contains the\n",
        "                   unique class labels corresponding to the integers in y\n",
        "    \"\"\"\n",
        "\n",
        "    x = []\n",
        "    y_labels = []\n",
        "    for line in open(filepath):\n",
        "        if line.strip() != \"\": # handle empty rows in file\n",
        "            row = line.strip().split(\",\")\n",
        "            x.append(list(map(float, row[:-1])))\n",
        "            y_labels.append(row[-1])\n",
        "\n",
        "    [classes, y] = np.unique(y_labels, return_inverse=True)\n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return (x, y, classes)\n",
        "\n",
        "\n",
        "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
        "    \"\"\" Split dataset into training and test sets, according to the given\n",
        "        test set proportion.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        test_proprotion (float): the desired proportion of test examples\n",
        "                                 (0.0-1.0)\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x_train, x_test, y_train, y_test)\n",
        "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
        "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
        "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
        "               - y_test (np.ndarray): Test labels, shape (N_train, )\n",
        "    \"\"\"\n",
        "\n",
        "    shuffled_indices = random_generator.permutation(len(x))\n",
        "    n_test = round(len(x) * test_proportion)\n",
        "    n_train = len(x) - n_test\n",
        "    x_train = x[shuffled_indices[:n_train]]\n",
        "    y_train = y[shuffled_indices[:n_train]]\n",
        "    x_test = x[shuffled_indices[n_train:]]\n",
        "    y_test = y[shuffled_indices[n_train:]]\n",
        "    return (x_train, x_test, y_train, y_test)\n",
        "\n",
        "\n",
        "\n",
        "(x, y, classes) = read_dataset(\"iris.data\")\n",
        "\n",
        "seed = 60012\n",
        "rg = default_rng(seed)\n",
        "x_train, x_test, y_train, y_test = split_dataset(x, y,\n",
        "                                                 test_proportion=0.2,\n",
        "                                                 random_generator=rg)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(120, 4)\n",
            "(30, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LNLsjjiwPJi"
      },
      "source": [
        "## K-NN Classifier\n",
        "\n",
        "In Lab 1, you have constructed a (one) Nearest Neighbour classifier.\n",
        "\n",
        "We will now try to generalise the nearest neighbour classifier as a **K-Nearest Neighours (K-NN)** classifier. For each test example, the classifier will predict the majority class label among the $K$ nearest training examples, again according to the Euclidean distance metric $d(x^{(i)}, x^{(q)})=\\sqrt{\\sum_f^F (x_f^{(i)} - x_f^{(q)})^2}$. If there is draw, choose one of the majority class labels arbitrarily, or at random.\n",
        "\n",
        "Complete the `predict()` method of the `KNNClassifier` class. Note that the class now takes an optional hyperparameter `k` in its constructor.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtnx_HjTyA2N"
      },
      "source": [
        "class KNNClassifier:\n",
        "    def __init__(self, k=5):\n",
        "        \"\"\" K-NN Classifier.\n",
        "\n",
        "        Args:\n",
        "        k (int): Number of nearest neighbours. Defaults to 5.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.x = np.array([])\n",
        "        self.y = np.array([])\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\" Fit the training data to the classifier.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\" Perform prediction given some examples.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "\n",
        "        Returns:\n",
        "        y (np.ndarray): Predicted class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for x_q in x:\n",
        "            # Compute the Euclidean distances between x_q and all training samples\n",
        "            distances = np.linalg.norm(self.x - x_q, axis=1)\n",
        "\n",
        "            # Get indices of the k-nearest neighbors\n",
        "            k_nearest_indices = distances.argsort()[:self.k]\n",
        "\n",
        "            # Get the corresponding labels of the k-nearest neighbors\n",
        "            k_nearest_labels = self.y[k_nearest_indices]\n",
        "\n",
        "            # Manually count occurrences of each label\n",
        "            label_count = {}\n",
        "            for label in k_nearest_labels:\n",
        "                if label in label_count:\n",
        "                    label_count[label] += 1\n",
        "                else:\n",
        "                    label_count[label] = 1\n",
        "\n",
        "            # Find the label with the maximum count (majority vote)\n",
        "            most_common_label = max(label_count, key=label_count.get)\n",
        "\n",
        "            #print(f\"from {label_count}, the most common label is {most_common_label}\")\n",
        "\n",
        "            # Append the predicted label to the list\n",
        "            predictions.append(most_common_label)\n",
        "\n",
        "        return np.array(predictions)\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBpJ8EjVy-bi",
        "outputId": "66c5afd2-28f6-4d3c-ee75-c10accf0c9bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nn_classifier = KNNClassifier(k=5)\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "nn_predictions = nn_classifier.predict(x_test)\n",
        "print(f\"The number of predicion is\", len(x_test))\n",
        "print(nn_predictions)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of predicion is 30\n",
            "[1 2 1 2 0 1 2 0 0 2 2 2 2 0 2 0 0 2 2 1 2 1 1 1 1 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4yZIOgepd2L"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Now, let's evaluate the accuracy of our K-NN classifier. We will test K from 1 to 20. Again, you should be able to achieve very high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfjzLLwUqDC-"
      },
      "source": [
        "def compute_accuracy(y_gold, y_prediction):\n",
        "    \"\"\" Compute the accuracy given the ground truth and predictions\n",
        "\n",
        "    Args:\n",
        "    y_gold (np.ndarray): the correct ground truth/gold standard labels\n",
        "    y_prediction (np.ndarray): the predicted labels\n",
        "\n",
        "    Returns:\n",
        "    float : the accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(y_gold) == len(y_prediction)\n",
        "\n",
        "    try:\n",
        "        return np.sum(y_gold == y_prediction) / len(y_gold)\n",
        "    except ZeroDivisionError:\n",
        "        return 0"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWRWsYurz1om",
        "outputId": "7deac36c-5a70-47b3-96c5-ea31f2d3b554",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for k in range(1, 21):\n",
        "    nn_classifier = KNNClassifier(k)\n",
        "    nn_classifier.fit(x_train, y_train)\n",
        "    nn_predictions = nn_classifier.predict(x_test)\n",
        "    accuracy = compute_accuracy(y_test, nn_predictions)\n",
        "    print(f\"K={k}: {accuracy}\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K=1: 0.9666666666666667\n",
            "K=2: 0.9666666666666667\n",
            "K=3: 1.0\n",
            "K=4: 1.0\n",
            "K=5: 1.0\n",
            "K=6: 1.0\n",
            "K=7: 0.9666666666666667\n",
            "K=8: 1.0\n",
            "K=9: 0.9666666666666667\n",
            "K=10: 1.0\n",
            "K=11: 1.0\n",
            "K=12: 1.0\n",
            "K=13: 0.9666666666666667\n",
            "K=14: 0.9666666666666667\n",
            "K=15: 0.9666666666666667\n",
            "K=16: 0.9666666666666667\n",
            "K=17: 0.9666666666666667\n",
            "K=18: 0.9666666666666667\n",
            "K=19: 0.9666666666666667\n",
            "K=20: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z8I2TXaLepZ"
      },
      "source": [
        "## Distance Weighted K-NN Classifier\n",
        "\n",
        "Now, let us implement the **distance weighted** K-NN classifier as discussed in the lectures.\n",
        "Pleae complete the `predict()` method for the `WeightedKNNClassifier` class below.\n",
        "\n",
        "For each test example $x^{(q)}$, you will need to compute the weight $w^{(i)}_{q}$ for each training example $x^{(i)}$.\n",
        "\n",
        "Then for each class label, sum the weights of the $K$ nearest examples. Assign the test example to the class label with largest sum.\n",
        "\n",
        "The weight can be anything reasonable. For this tutorial, we will use the weight $w_{q}^{(i)}=\\frac{1}{d(x^{(i)}, x^{(q)})}$, where $d(x^{(i)}, x^{(q)})=\\sqrt{\\sum_f^F (x_f^{(i)} - x_f^{(q)})^2}$ is the Euclidean distance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTQ-cgA8LTFB"
      },
      "source": [
        "class WeightedKNNClassifier:\n",
        "    def __init__(self, k=5):\n",
        "        \"\"\" K-NN Classifier.\n",
        "\n",
        "        Args:\n",
        "        k (int): Number of nearest neighbours. Defaults to 5.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.x = np.array([])\n",
        "        self.y = np.array([])\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        \"\"\" Fit the training data to the classifier.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\" Perform prediction given some examples.\n",
        "\n",
        "        Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "\n",
        "        Returns:\n",
        "        y (np.ndarray): Predicted class labels, numpy array with shape (N,)\n",
        "        \"\"\"\n",
        "\n",
        "        predictions = []\n",
        "        epsilon = 1e-8  # Small value to avoid division by zero\n",
        "\n",
        "        for x_q in x:\n",
        "            # Compute the Euclidean distances between x_q and all training samples\n",
        "            distances = np.linalg.norm(self.x - x_q, axis=1)\n",
        "\n",
        "            # Get indices of the k-nearest neighbors\n",
        "            k_nearest_indices = distances.argsort()[:self.k]\n",
        "\n",
        "            # Get the corresponding labels of the k-nearest neighbors\n",
        "            k_nearest_labels = self.y[k_nearest_indices]\n",
        "\n",
        "            # Calculate weights for each of the k-nearest neighbors (1/distance)\n",
        "            weights = 1 / (distances[k_nearest_indices] + epsilon)\n",
        "\n",
        "            # Sum weights for each class\n",
        "            label_weight_sum = {}\n",
        "            for idx, label in enumerate(k_nearest_labels):\n",
        "                weight = weights[idx]\n",
        "                if label in label_weight_sum:\n",
        "                    label_weight_sum[label] += weight\n",
        "                else:\n",
        "                    label_weight_sum[label] = weight\n",
        "\n",
        "            # Assign the label with the largest sum of weights\n",
        "            most_weighted_label = max(label_weight_sum, key=label_weight_sum.get)\n",
        "\n",
        "            #print(f\"from {label_weight_sum}, the most common label is {most_weighted_label}\")\n",
        "\n",
        "            # Append the predicted label to the list\n",
        "            predictions.append(most_weighted_label)\n",
        "\n",
        "        return np.array(predictions)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gjm_y7NnLTFP",
        "outputId": "579676ab-f676-49c3-cd1f-e1c25aaa02ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nn_classifier = WeightedKNNClassifier(k=5)\n",
        "nn_classifier.fit(x_train, y_train)\n",
        "nn_predictions = nn_classifier.predict(x_test)\n",
        "print(nn_predictions)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from {1: 21.257749757409382}, the most common label is 1\n",
            "from {2: 14.036607432045065}, the most common label is 2\n",
            "from {1: 18.13738269483285}, the most common label is 1\n",
            "from {2: 6.683973520038626}, the most common label is 2\n",
            "from {0: 20.388747889543048}, the most common label is 0\n",
            "from {1: 18.45177885310918}, the most common label is 1\n",
            "from {2: 11.911945220285627}, the most common label is 2\n",
            "from {0: 21.802061640227592}, the most common label is 0\n",
            "from {0: 26.480199354198458}, the most common label is 0\n",
            "from {2: 17.36556842607809}, the most common label is 2\n",
            "from {2: 16.26543096875609}, the most common label is 2\n",
            "from {2: 8.782166701217106}, the most common label is 2\n",
            "from {2: 15.973507708272594, 1: 2.357022548399605}, the most common label is 2\n",
            "from {0: 31.458840049159}, the most common label is 0\n",
            "from {2: 12.435364645605574}, the most common label is 2\n",
            "from {0: 200000016.54700446}, the most common label is 0\n",
            "from {0: 19.13002384755861}, the most common label is 0\n",
            "from {2: 12.873652933458605}, the most common label is 2\n",
            "from {1: 1.7960529880096847, 2: 5.983650202956062}, the most common label is 2\n",
            "from {1: 8.191912456263662, 2: 4.782378739939404}, the most common label is 1\n",
            "from {2: 13.521847289946196}, the most common label is 2\n",
            "from {1: 12.352819506762387}, the most common label is 1\n",
            "from {1: 15.804729884311097, 2: 2.2941572860740402}, the most common label is 1\n",
            "from {1: 9.045820303529673}, the most common label is 1\n",
            "from {1: 13.195594943180314}, the most common label is 1\n",
            "from {0: 12.601093142932806}, the most common label is 0\n",
            "from {0: 22.492483811542858}, the most common label is 0\n",
            "from {0: 13.929966700187032}, the most common label is 0\n",
            "from {0: 21.599861081517048}, the most common label is 0\n",
            "from {0: 6.516127188365081}, the most common label is 0\n",
            "[1 2 1 2 0 1 2 0 0 2 2 2 2 0 2 0 0 2 2 1 2 1 1 1 1 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCfaQ5WmgqSa"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Again, we will evaluate the accuracy of the distance weighted K-NN classifier for K=1 to 20. As this is a small and relatively easy dataset, you should get similar results as with the simple K-NN classifier.\n",
        "\n",
        "You might observe the benefits of the distance weighted K-NN classifier better when training on larger and noisier datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPr1YEfJ3g8Z",
        "outputId": "6af6d053-d390-4dbb-e47b-c5c7962ca246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for k in range(1, 21):\n",
        "    nn_classifier = WeightedKNNClassifier(k)\n",
        "    nn_classifier.fit(x_train, y_train)\n",
        "    nn_predictions = nn_classifier.predict(x_test)\n",
        "    accuracy = compute_accuracy(y_test, nn_predictions)\n",
        "    print(f\"K={k}: {accuracy}\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K=1: 0.9666666666666667\n",
            "K=2: 0.9666666666666667\n",
            "K=3: 1.0\n",
            "K=4: 1.0\n",
            "K=5: 1.0\n",
            "K=6: 1.0\n",
            "K=7: 0.9666666666666667\n",
            "K=8: 1.0\n",
            "K=9: 0.9666666666666667\n",
            "K=10: 1.0\n",
            "K=11: 1.0\n",
            "K=12: 1.0\n",
            "K=13: 0.9666666666666667\n",
            "K=14: 0.9666666666666667\n",
            "K=15: 0.9666666666666667\n",
            "K=16: 0.9666666666666667\n",
            "K=17: 0.9666666666666667\n",
            "K=18: 0.9666666666666667\n",
            "K=19: 0.9666666666666667\n",
            "K=20: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMajcyH7Vv5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "You have built a K-Nearest Neighbour classifier (and the distance-weighted variant)! Because the Iris dataset is a small and simple dataset, you may only observe a small improvement in accuracy (if any) compared to a simple one-nearest neighbour classifier.\n",
        "\n",
        "And that is it for this week's lab tutorials. We will not have a decision tree tutorial for this course. The coursework itself will contain a short guide to help you think about implementing your own decision trees.\n",
        "\n",
        "In the next lab tutorial, you will be implementing different evaluation metrics and performing cross-validation.\n"
      ]
    }
  ]
}