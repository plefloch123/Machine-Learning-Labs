{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MBmOdXoenFw"
      },
      "source": [
        "# Lab 4: Simple Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3iS23ADfo4w"
      },
      "source": [
        "## Version history\n",
        "\n",
        "| Date | Author | Description |\n",
        "|:----:|:------:|:------------|\n",
        "2021-02-03 | Josiah Wang | First version |\n",
        "2021-11-13 | Josiah Wang | Optimisation with derivatives: $x$ should be $x^{(i)}$ |\n",
        "2022-11-02 | Josiah Wang | Updated to `ax = fig.add_subplot(projection='3d')`. The original `fig.gca(projection='3d')` is deprecated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EbbkgqOgZK_"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The aim of this lab exercise is for you to gain some experience implementing and training a simple linear regression model from scratch. This will help you improve your understanding of linear regression and machine learning optimisation.\n",
        "\n",
        "By the end of this lab exercise, you will have\n",
        "- implemented a simple linear regression model\n",
        "- defined and implemented a loss function\n",
        "- optimised the parameters of your model using gradient descent\n",
        "\n",
        "There will be a bit less coding required on your side in this exercise compared to previous exercises. The aim is for you to try to really understand linear regression at implementation level to complement the lectures, which will help you in future weeks as you move on to Neural Networks in your coursework assignments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0iaMjnIWEpe"
      },
      "source": [
        "## Simple Linear Regression\n",
        "\n",
        "In this tutorial, we will focus on the **regression task**. For simplicity, we will implement a *simple linear regression* model with one input variable and one output variable. More specifically, our task is to predict the value of $y$ given the input $x$.\n",
        "\n",
        "Let us develop our simple linear regressor with a simple toy example to make sure that our model works correctly. You can later apply it to a bigger dataset if desired."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w2N6AZsYJUd",
        "outputId": "dc2b102c-ba86-426b-8881-60f33f462733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# toy dataset\n",
        "x_train = np.array([1.0, 1.2, 2.0, 3.5, 4.0, 5.0])\n",
        "y_train = np.array([3.1, 3.5, 5.0, 7.9, 9.1, 10.9])\n",
        "x_test = np.array([2.5, 3.0, 4.5])\n",
        "y_test = np.array([6.0, 7.0, 10.1])\n",
        "\n",
        "# plot toy data\n",
        "plt.scatter(x_train, y_train, c=\"blue\")\n",
        "plt.scatter(x_test, y_test, c=\"red\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YagIkbxvaC-b"
      },
      "source": [
        "### Model\n",
        "\n",
        "As you should be able to see from the plot, the toy dataset can almost be perfectly modelled with a straight line. The model should also be able to pretty accurately predict the value of $y$ of the test set.\n",
        "\n",
        "Assuming you still remember your high school Maths, a straight line is represented as $y = wx + b$, where $w$ is the slope and $b$ the intercept/bias. Our objective is to find the line that best fits our training data. More specifically, we want our regressor to automatically learn the parameters $w$ and $b$ such that we can accurately predict the real-valued label ${\\hat y}$ given an example $x$. The objective is to get ${\\hat y}$ to be as close as possible to the values of $y$ of the training data (and presumably the true $y$).\n",
        "\n",
        "Let us now build our simple linear regression model. Complete the `forward()` method of the `SimpleLinearRegression` class below to return the value of the output $y$ given an input `x` and the current weight `w` and bias `b` of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLNpOSjXUQrY",
        "outputId": "c2582a3c-b1ec-4785-ec7c-e983cef7fd31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from numpy.random import default_rng\n",
        "\n",
        "class SimpleLinearRegression:\n",
        "    def __init__(self, random_generator=default_rng()):\n",
        "        # initialise the slope with a random value drawn from a standard normal\n",
        "        # distribution (mean=0, stddev=1)\n",
        "        self.w = random_generator.standard_normal()\n",
        "\n",
        "        # initialise bias to 0\n",
        "        self.b = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Perform forward pass given an input x\n",
        "\n",
        "        Args:\n",
        "            x (float): input instance\n",
        "\n",
        "        Returns:\n",
        "            float: the output of the model given the current weights\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Complete this\n",
        "        return self.w * x + self.b\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        \"\"\" Placeholder for later\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "## Quick test: This should return 8\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2\n",
        "y_hat = model.forward(x)\n",
        "print(y_hat) # should print 8"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9mtsqFXBNNa"
      },
      "source": [
        "### Loss function\n",
        "\n",
        "From the plot and the numbers, you may have manually worked out that the 'best' line would be $\\hat{y} \\approx 2x+1$, that is, the optimal parameter values are $w \\approx 2$ and $b \\approx 1$.\n",
        "\n",
        "What constitutes the 'best' line? We will first have to define what 'best' actually means. Intuitively, the 'best' line would be the one that goes through all training points as closely as possible.\n",
        "\n",
        "To enable our model to automatically learn what the parameter values of the 'best' line are from training examples, we will have to formally define that we mean by 'best'. We define this via a *loss function* (or cost function). For this tutorial, we will use the loss function as in the lectures - the **sum of squared differences** between the predicted label vs. the ground truth label across the training instances.\n",
        "\n",
        "$$L = \\frac{1}{2} \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 = \\frac{1}{2} \\sum_{i=1}^{N} \\left(wx^{(i)} + b - y^{(i)}\\right)^2$$\n",
        "\n",
        "Our objective is to select the parameters $\\theta = \\{ w, b \\}$ that **minimise** the loss (or error).\n",
        "\n",
        "$$\\theta = argmin_{\\theta} \\, L$$\n",
        "\n",
        "To make things easy, let us implement the loss function for a **single** instance $x$:\n",
        "\n",
        "$$L^{(i)} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2$$\n",
        "\n",
        "Complete the `loss()` method of `SimpleLinearRegression` below to return the individual loss for an instance `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZM0MAgiE2UZ",
        "outputId": "fc9368d6-9b17-44ec-826a-e685330cc8e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Loss method for SimpleLinearRegression\n",
        "def loss(self, x, y):\n",
        "    \"\"\" Compute the loss for an input x\n",
        "\n",
        "    Args:\n",
        "        x (float): input instance\n",
        "        y (float): ground truth output\n",
        "\n",
        "    Returns:\n",
        "        float: the model loss for an instance x\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Complete this\n",
        "    y_hat = self.forward(x)\n",
        "    return (y_hat - y)**2\n",
        "\n",
        "\n",
        "# A quick hack to bind this function as the SimpleLinearRegression.loss() method\n",
        "SimpleLinearRegression.loss = loss\n",
        "\n",
        "\n",
        "## Quick test: This should return 0.25\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2.0\n",
        "y = 8.5\n",
        "test_loss = model.loss(x, y)\n",
        "print(test_loss) # should print 0.25"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxwg9EPiGu2l"
      },
      "source": [
        "### Optimisation by brute force search\n",
        "\n",
        "Now, how do we get the model to automatically figure out the optimal parameters from training data? Remember that the optimal parameter values are the ones that minimise the loss function. A naive approach would be to compute the loss for different combinations of $w$ and $b$ and selecting the combination that results in the smallest loss.\n",
        "\n",
        "The code below will search for $w$ between $0$ and $4$, and for $b$ between $0$ and $2$ to find the best combination of $w$ and $b$. Examine the code, and try to understand what it is doing, then run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyV8OCZSIdrj",
        "outputId": "d92d1444-4000-408c-fd8b-816fa03265d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "# to store all losses for later use\n",
        "losses = []\n",
        "\n",
        "# the parameters to search\n",
        "weights = np.arange(0, 4.1, 0.2)\n",
        "biases = np.arange(0, 2.1, 0.2)\n",
        "\n",
        "# for storing the loss in a matrix for visualisation later\n",
        "loss_matrix = np.zeros((len(weights), len(biases)))\n",
        "\n",
        "# compute loss for each (w,b) combination\n",
        "for i, w in enumerate(weights):\n",
        "    for j, b in enumerate(biases):\n",
        "        print(f\"(w={w:.1f}, b={b:.1f})\")\n",
        "\n",
        "        # setup weights of model\n",
        "        model.w = w\n",
        "        model.b = b\n",
        "\n",
        "        sum_loss = 0\n",
        "        # for each example\n",
        "        for (x, y) in zip(x_train, y_train):\n",
        "            # compute the loss for this example\n",
        "            single_loss = model.loss(x, y)\n",
        "\n",
        "            # and add it to the sum\n",
        "            sum_loss += single_loss\n",
        "\n",
        "            # print out the values just to make sure everything is working correctly\n",
        "            y_hat = model.forward(x)\n",
        "            print(f\"    x: {x}, y: {y}, y_hat: {y_hat:.1f}, loss: {single_loss:.2f}\")\n",
        "\n",
        "        # print out the sum of individual losses\n",
        "        # I multiplied by 0.5 to be consistent with the equation earlier,\n",
        "        # but this is not necessary in practice as this is a constant\n",
        "        print(f\"    Loss = {(0.5 * sum_loss):.4f}\\n\")\n",
        "\n",
        "        # store the losses and the corresponding (w,b) for later use\n",
        "        losses.append((0.5*sum_loss, w, b))\n",
        "\n",
        "        # store the losses in a matrix form for visualisation later\n",
        "        loss_matrix[i,j] = 0.5 * sum_loss\n",
        "\n",
        "# find combination with minimum loss\n",
        "(min_loss, best_w, best_b) = min(losses, key=lambda x:x[0])\n",
        "print(\"BEST:\")\n",
        "print(f\"w={best_w}, b={best_b}, loss={min_loss:.4f}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(w=0.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.0, loss: 9.61\n",
            "    x: 1.2, y: 3.5, y_hat: 0.0, loss: 12.25\n",
            "    x: 2.0, y: 5.0, y_hat: 0.0, loss: 25.00\n",
            "    x: 3.5, y: 7.9, y_hat: 0.0, loss: 62.41\n",
            "    x: 4.0, y: 9.1, y_hat: 0.0, loss: 82.81\n",
            "    x: 5.0, y: 10.9, y_hat: 0.0, loss: 118.81\n",
            "    Loss = 155.4450\n",
            "\n",
            "(w=0.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.2, loss: 8.41\n",
            "    x: 1.2, y: 3.5, y_hat: 0.2, loss: 10.89\n",
            "    x: 2.0, y: 5.0, y_hat: 0.2, loss: 23.04\n",
            "    x: 3.5, y: 7.9, y_hat: 0.2, loss: 59.29\n",
            "    x: 4.0, y: 9.1, y_hat: 0.2, loss: 79.21\n",
            "    x: 5.0, y: 10.9, y_hat: 0.2, loss: 114.49\n",
            "    Loss = 147.6650\n",
            "\n",
            "(w=0.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.4, loss: 9.61\n",
            "    x: 2.0, y: 5.0, y_hat: 0.4, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 0.4, loss: 56.25\n",
            "    x: 4.0, y: 9.1, y_hat: 0.4, loss: 75.69\n",
            "    x: 5.0, y: 10.9, y_hat: 0.4, loss: 110.25\n",
            "    Loss = 140.1250\n",
            "\n",
            "(w=0.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.6, loss: 8.41\n",
            "    x: 2.0, y: 5.0, y_hat: 0.6, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 0.6, loss: 53.29\n",
            "    x: 4.0, y: 9.1, y_hat: 0.6, loss: 72.25\n",
            "    x: 5.0, y: 10.9, y_hat: 0.6, loss: 106.09\n",
            "    Loss = 132.8250\n",
            "\n",
            "(w=0.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.8, loss: 7.29\n",
            "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 0.8, loss: 50.41\n",
            "    x: 4.0, y: 9.1, y_hat: 0.8, loss: 68.89\n",
            "    x: 5.0, y: 10.9, y_hat: 0.8, loss: 102.01\n",
            "    Loss = 125.7650\n",
            "\n",
            "(w=0.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.25\n",
            "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 1.0, loss: 47.61\n",
            "    x: 4.0, y: 9.1, y_hat: 1.0, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 1.0, loss: 98.01\n",
            "    Loss = 118.9450\n",
            "\n",
            "(w=0.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.29\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 1.2, loss: 44.89\n",
            "    x: 4.0, y: 9.1, y_hat: 1.2, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 1.2, loss: 94.09\n",
            "    Loss = 112.3650\n",
            "\n",
            "(w=0.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 1.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 1.4, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 1.4, loss: 90.25\n",
            "    Loss = 106.0250\n",
            "\n",
            "(w=0.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 1.6, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 1.6, loss: 86.49\n",
            "    Loss = 99.9250\n",
            "\n",
            "(w=0.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 1.8, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 1.8, loss: 82.81\n",
            "    Loss = 94.0650\n",
            "\n",
            "(w=0.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.0, loss: 34.81\n",
            "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
            "    Loss = 88.4450\n",
            "\n",
            "(w=0.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.2, loss: 8.41\n",
            "    x: 1.2, y: 3.5, y_hat: 0.2, loss: 10.63\n",
            "    x: 2.0, y: 5.0, y_hat: 0.4, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 0.7, loss: 51.84\n",
            "    x: 4.0, y: 9.1, y_hat: 0.8, loss: 68.89\n",
            "    x: 5.0, y: 10.9, y_hat: 1.0, loss: 98.01\n",
            "    Loss = 129.4688\n",
            "\n",
            "(w=0.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.4, loss: 9.36\n",
            "    x: 2.0, y: 5.0, y_hat: 0.6, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 0.9, loss: 49.00\n",
            "    x: 4.0, y: 9.1, y_hat: 1.0, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 1.2, loss: 94.09\n",
            "    Loss = 122.3568\n",
            "\n",
            "(w=0.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.6, loss: 8.18\n",
            "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 1.1, loss: 46.24\n",
            "    x: 4.0, y: 9.1, y_hat: 1.2, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 1.4, loss: 90.25\n",
            "    Loss = 115.4848\n",
            "\n",
            "(w=0.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.8, loss: 7.08\n",
            "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 1.3, loss: 43.56\n",
            "    x: 4.0, y: 9.1, y_hat: 1.4, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 1.6, loss: 86.49\n",
            "    Loss = 108.8528\n",
            "\n",
            "(w=0.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.05\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 1.5, loss: 40.96\n",
            "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 1.8, loss: 82.81\n",
            "    Loss = 102.4608\n",
            "\n",
            "(w=0.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.11\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 1.7, loss: 38.44\n",
            "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
            "    Loss = 96.3088\n",
            "\n",
            "(w=0.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.24\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 1.9, loss: 36.00\n",
            "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 2.2, loss: 75.69\n",
            "    Loss = 90.3968\n",
            "\n",
            "(w=0.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 2.1, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 2.2, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 2.4, loss: 72.25\n",
            "    Loss = 84.7248\n",
            "\n",
            "(w=0.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.3, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 2.6, loss: 68.89\n",
            "    Loss = 79.2928\n",
            "\n",
            "(w=0.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 2.5, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 2.8, loss: 65.61\n",
            "    Loss = 74.1008\n",
            "\n",
            "(w=0.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 2.7, loss: 27.04\n",
            "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
            "    Loss = 69.1488\n",
            "\n",
            "(w=0.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.4, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.5, loss: 9.12\n",
            "    x: 2.0, y: 5.0, y_hat: 0.8, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 1.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 1.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 2.0, loss: 79.21\n",
            "    Loss = 105.8802\n",
            "\n",
            "(w=0.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.7, loss: 7.95\n",
            "    x: 2.0, y: 5.0, y_hat: 1.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 1.6, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 1.8, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 2.2, loss: 75.69\n",
            "    Loss = 99.4362\n",
            "\n",
            "(w=0.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.9, loss: 6.86\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 1.8, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 2.0, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 2.4, loss: 72.25\n",
            "    Loss = 93.2322\n",
            "\n",
            "(w=0.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.1, loss: 5.86\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 2.0, loss: 34.81\n",
            "    x: 4.0, y: 9.1, y_hat: 2.2, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 2.6, loss: 68.89\n",
            "    Loss = 87.2682\n",
            "\n",
            "(w=0.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.3, loss: 4.93\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 2.2, loss: 32.49\n",
            "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 2.8, loss: 65.61\n",
            "    Loss = 81.5442\n",
            "\n",
            "(w=0.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.5, loss: 4.08\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 2.4, loss: 30.25\n",
            "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
            "    Loss = 76.0602\n",
            "\n",
            "(w=0.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.31\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.6, loss: 28.09\n",
            "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 3.2, loss: 59.29\n",
            "    Loss = 70.8162\n",
            "\n",
            "(w=0.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 2.8, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.0, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 3.4, loss: 56.25\n",
            "    Loss = 65.8122\n",
            "\n",
            "(w=0.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.0, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 3.6, loss: 53.29\n",
            "    Loss = 61.0482\n",
            "\n",
            "(w=0.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.2, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 3.8, loss: 50.41\n",
            "    Loss = 56.5242\n",
            "\n",
            "(w=0.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.4, loss: 20.25\n",
            "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
            "    Loss = 52.2402\n",
            "\n",
            "(w=0.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 0.7, loss: 7.73\n",
            "    x: 2.0, y: 5.0, y_hat: 1.2, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 2.1, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 2.4, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 3.0, loss: 62.41\n",
            "    Loss = 84.6792\n",
            "\n",
            "(w=0.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 0.9, loss: 6.66\n",
            "    x: 2.0, y: 5.0, y_hat: 1.4, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 2.3, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 2.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 3.2, loss: 59.29\n",
            "    Loss = 78.9032\n",
            "\n",
            "(w=0.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.1, loss: 5.66\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 2.5, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 2.8, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 3.4, loss: 56.25\n",
            "    Loss = 73.3672\n",
            "\n",
            "(w=0.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.3, loss: 4.75\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 2.7, loss: 27.04\n",
            "    x: 4.0, y: 9.1, y_hat: 3.0, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 3.6, loss: 53.29\n",
            "    Loss = 68.0712\n",
            "\n",
            "(w=0.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.5, loss: 3.92\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 2.9, loss: 25.00\n",
            "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 3.8, loss: 50.41\n",
            "    Loss = 63.0152\n",
            "\n",
            "(w=0.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.17\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.1, loss: 23.04\n",
            "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
            "    Loss = 58.1992\n",
            "\n",
            "(w=0.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.50\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.3, loss: 21.16\n",
            "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 4.2, loss: 44.89\n",
            "    Loss = 53.6232\n",
            "\n",
            "(w=0.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.5, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 3.8, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 4.4, loss: 42.25\n",
            "    Loss = 49.2872\n",
            "\n",
            "(w=0.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.7, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 4.6, loss: 39.69\n",
            "    Loss = 45.1912\n",
            "\n",
            "(w=0.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 3.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 4.8, loss: 37.21\n",
            "    Loss = 41.3352\n",
            "\n",
            "(w=0.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 4.1, loss: 14.44\n",
            "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
            "    Loss = 37.7192\n",
            "\n",
            "(w=0.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 0.8, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 1.0, loss: 6.45\n",
            "    x: 2.0, y: 5.0, y_hat: 1.6, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 2.8, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.2, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 4.0, loss: 47.61\n",
            "    Loss = 65.8658\n",
            "\n",
            "(w=0.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.48\n",
            "    x: 2.0, y: 5.0, y_hat: 1.8, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 3.0, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 3.4, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 4.2, loss: 44.89\n",
            "    Loss = 60.7578\n",
            "\n",
            "(w=0.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.58\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 3.2, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 3.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 4.4, loss: 42.25\n",
            "    Loss = 55.8898\n",
            "\n",
            "(w=0.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.76\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.4, loss: 20.25\n",
            "    x: 4.0, y: 9.1, y_hat: 3.8, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 4.6, loss: 39.69\n",
            "    Loss = 51.2618\n",
            "\n",
            "(w=0.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 3.03\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.6, loss: 18.49\n",
            "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 4.8, loss: 37.21\n",
            "    Loss = 46.8738\n",
            "\n",
            "(w=0.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.37\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.8, loss: 16.81\n",
            "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
            "    Loss = 42.7258\n",
            "\n",
            "(w=0.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.80\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.0, loss: 15.21\n",
            "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 5.2, loss: 32.49\n",
            "    Loss = 38.8178\n",
            "\n",
            "(w=0.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 4.2, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 4.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 5.4, loss: 30.25\n",
            "    Loss = 35.1498\n",
            "\n",
            "(w=0.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 4.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 5.6, loss: 28.09\n",
            "    Loss = 31.7218\n",
            "\n",
            "(w=0.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 4.6, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 5.8, loss: 26.01\n",
            "    Loss = 28.5338\n",
            "\n",
            "(w=0.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 4.8, loss: 9.61\n",
            "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
            "    Loss = 25.5858\n",
            "\n",
            "(w=1.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.0, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 1.2, loss: 5.29\n",
            "    x: 2.0, y: 5.0, y_hat: 2.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 3.5, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 4.0, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 5.0, loss: 34.81\n",
            "    Loss = 49.4400\n",
            "\n",
            "(w=1.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 2.2, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 3.7, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 4.2, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 5.2, loss: 32.49\n",
            "    Loss = 45.0000\n",
            "\n",
            "(w=1.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 3.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 4.4, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 5.4, loss: 30.25\n",
            "    Loss = 40.8000\n",
            "\n",
            "(w=1.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 4.1, loss: 14.44\n",
            "    x: 4.0, y: 9.1, y_hat: 4.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 5.6, loss: 28.09\n",
            "    Loss = 36.8400\n",
            "\n",
            "(w=1.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.3, loss: 12.96\n",
            "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 5.8, loss: 26.01\n",
            "    Loss = 33.1200\n",
            "\n",
            "(w=1.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.69\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 4.5, loss: 11.56\n",
            "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
            "    Loss = 29.6400\n",
            "\n",
            "(w=1.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.21\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 4.7, loss: 10.24\n",
            "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 6.2, loss: 22.09\n",
            "    Loss = 26.4000\n",
            "\n",
            "(w=1.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 4.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 5.4, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 6.4, loss: 20.25\n",
            "    Loss = 23.4000\n",
            "\n",
            "(w=1.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 5.1, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 6.6, loss: 18.49\n",
            "    Loss = 20.6400\n",
            "\n",
            "(w=1.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 5.3, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 6.8, loss: 16.81\n",
            "    Loss = 18.1200\n",
            "\n",
            "(w=1.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 5.5, loss: 5.76\n",
            "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
            "    Loss = 15.8400\n",
            "\n",
            "(w=1.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.2, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 1.4, loss: 4.24\n",
            "    x: 2.0, y: 5.0, y_hat: 2.4, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 4.2, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 4.8, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 6.0, loss: 24.01\n",
            "    Loss = 35.4018\n",
            "\n",
            "(w=1.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.6, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 2.6, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 4.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 5.0, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 6.2, loss: 22.09\n",
            "    Loss = 31.6298\n",
            "\n",
            "(w=1.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.8, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.6, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 5.2, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 6.4, loss: 20.25\n",
            "    Loss = 28.0978\n",
            "\n",
            "(w=1.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 4.8, loss: 9.61\n",
            "    x: 4.0, y: 9.1, y_hat: 5.4, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 6.6, loss: 18.49\n",
            "    Loss = 24.8058\n",
            "\n",
            "(w=1.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 5.0, loss: 8.41\n",
            "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 6.8, loss: 16.81\n",
            "    Loss = 21.7538\n",
            "\n",
            "(w=1.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.12\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 5.2, loss: 7.29\n",
            "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
            "    Loss = 18.9418\n",
            "\n",
            "(w=1.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.74\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 5.4, loss: 6.25\n",
            "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 7.2, loss: 13.69\n",
            "    Loss = 16.3698\n",
            "\n",
            "(w=1.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 5.6, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 6.2, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 7.4, loss: 12.25\n",
            "    Loss = 14.0378\n",
            "\n",
            "(w=1.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 5.8, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 7.6, loss: 10.89\n",
            "    Loss = 11.9458\n",
            "\n",
            "(w=1.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.0, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 7.8, loss: 9.61\n",
            "    Loss = 10.0938\n",
            "\n",
            "(w=1.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 6.2, loss: 2.89\n",
            "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
            "    Loss = 8.4818\n",
            "\n",
            "(w=1.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.4, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 1.7, loss: 3.31\n",
            "    x: 2.0, y: 5.0, y_hat: 2.8, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 4.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 5.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 7.0, loss: 15.21\n",
            "    Loss = 23.7512\n",
            "\n",
            "(w=1.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 3.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 5.1, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 5.8, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 7.2, loss: 13.69\n",
            "    Loss = 20.6472\n",
            "\n",
            "(w=1.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 5.3, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 6.0, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 7.4, loss: 12.25\n",
            "    Loss = 17.7832\n",
            "\n",
            "(w=1.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 5.5, loss: 5.76\n",
            "    x: 4.0, y: 9.1, y_hat: 6.2, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 7.6, loss: 10.89\n",
            "    Loss = 15.1592\n",
            "\n",
            "(w=1.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 5.7, loss: 4.84\n",
            "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 7.8, loss: 9.61\n",
            "    Loss = 12.7752\n",
            "\n",
            "(w=1.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.67\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 5.9, loss: 4.00\n",
            "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
            "    Loss = 10.6312\n",
            "\n",
            "(w=1.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.38\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 6.1, loss: 3.24\n",
            "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.2, loss: 7.29\n",
            "    Loss = 8.7272\n",
            "\n",
            "(w=1.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.3, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 7.0, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 8.4, loss: 6.25\n",
            "    Loss = 7.0632\n",
            "\n",
            "(w=1.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 6.5, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 8.6, loss: 5.29\n",
            "    Loss = 5.6392\n",
            "\n",
            "(w=1.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 6.7, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 8.8, loss: 4.41\n",
            "    Loss = 4.4552\n",
            "\n",
            "(w=1.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 6.9, loss: 1.00\n",
            "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
            "    Loss = 3.5112\n",
            "\n",
            "(w=1.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 1.9, loss: 2.50\n",
            "    x: 2.0, y: 5.0, y_hat: 3.2, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 5.6, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 6.4, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.0, loss: 8.41\n",
            "    Loss = 14.4882\n",
            "\n",
            "(w=1.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.1, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 3.4, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 5.8, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 6.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 8.2, loss: 7.29\n",
            "    Loss = 12.0522\n",
            "\n",
            "(w=1.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.3, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 6.0, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 6.8, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 8.4, loss: 6.25\n",
            "    Loss = 9.8562\n",
            "\n",
            "(w=1.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 6.2, loss: 2.89\n",
            "    x: 4.0, y: 9.1, y_hat: 7.0, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 8.6, loss: 5.29\n",
            "    Loss = 7.9002\n",
            "\n",
            "(w=1.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.7, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 6.4, loss: 2.25\n",
            "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 8.8, loss: 4.41\n",
            "    Loss = 6.1842\n",
            "\n",
            "(w=1.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.34\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.6, loss: 1.69\n",
            "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
            "    Loss = 4.7082\n",
            "\n",
            "(w=1.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.14\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 6.8, loss: 1.21\n",
            "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 9.2, loss: 2.89\n",
            "    Loss = 3.4722\n",
            "\n",
            "(w=1.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.0, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 7.8, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 9.4, loss: 2.25\n",
            "    Loss = 2.4762\n",
            "\n",
            "(w=1.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.2, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 9.6, loss: 1.69\n",
            "    Loss = 1.7202\n",
            "\n",
            "(w=1.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 7.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 9.8, loss: 1.21\n",
            "    Loss = 1.2042\n",
            "\n",
            "(w=1.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.6, loss: 0.09\n",
            "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
            "    Loss = 0.9282\n",
            "\n",
            "(w=1.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 1.8, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 2.2, loss: 1.80\n",
            "    x: 2.0, y: 5.0, y_hat: 3.6, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 6.3, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 7.2, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 9.0, loss: 3.61\n",
            "    Loss = 7.6128\n",
            "\n",
            "(w=1.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 3.8, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 6.5, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 7.4, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 9.2, loss: 2.89\n",
            "    Loss = 5.8448\n",
            "\n",
            "(w=1.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 6.7, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 7.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 9.4, loss: 2.25\n",
            "    Loss = 4.3168\n",
            "\n",
            "(w=1.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 6.9, loss: 1.00\n",
            "    x: 4.0, y: 9.1, y_hat: 7.8, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 9.6, loss: 1.69\n",
            "    Loss = 3.0288\n",
            "\n",
            "(w=1.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 7.1, loss: 0.64\n",
            "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 9.8, loss: 1.21\n",
            "    Loss = 1.9808\n",
            "\n",
            "(w=1.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.12\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.3, loss: 0.36\n",
            "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
            "    Loss = 1.1728\n",
            "\n",
            "(w=1.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.02\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.5, loss: 0.16\n",
            "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 10.2, loss: 0.49\n",
            "    Loss = 0.6048\n",
            "\n",
            "(w=1.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 7.7, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 8.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 10.4, loss: 0.25\n",
            "    Loss = 0.2768\n",
            "\n",
            "(w=1.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.9, loss: 0.00\n",
            "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 10.6, loss: 0.09\n",
            "    Loss = 0.1888\n",
            "\n",
            "(w=1.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 8.1, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 10.8, loss: 0.01\n",
            "    Loss = 0.3408\n",
            "\n",
            "(w=1.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 8.3, loss: 0.16\n",
            "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
            "    Loss = 0.7328\n",
            "\n",
            "(w=2.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.0, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 2.4, loss: 1.21\n",
            "    x: 2.0, y: 5.0, y_hat: 4.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 7.0, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 8.0, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 10.0, loss: 0.81\n",
            "    Loss = 3.1250\n",
            "\n",
            "(w=2.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 4.2, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 7.2, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 8.2, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 10.2, loss: 0.49\n",
            "    Loss = 2.0250\n",
            "\n",
            "(w=2.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 7.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 8.4, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 10.4, loss: 0.25\n",
            "    Loss = 1.1650\n",
            "\n",
            "(w=2.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.6, loss: 0.09\n",
            "    x: 4.0, y: 9.1, y_hat: 8.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 10.6, loss: 0.09\n",
            "    Loss = 0.5450\n",
            "\n",
            "(w=2.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 7.8, loss: 0.01\n",
            "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 10.8, loss: 0.01\n",
            "    Loss = 0.1650\n",
            "\n",
            "(w=2.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.01\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 8.0, loss: 0.01\n",
            "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
            "    Loss = 0.0250\n",
            "\n",
            "(w=2.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.01\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.2, loss: 0.09\n",
            "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.2, loss: 0.09\n",
            "    Loss = 0.1250\n",
            "\n",
            "(w=2.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 8.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 9.4, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 11.4, loss: 0.25\n",
            "    Loss = 0.4650\n",
            "\n",
            "(w=2.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 8.6, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 11.6, loss: 0.49\n",
            "    Loss = 1.0450\n",
            "\n",
            "(w=2.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 8.8, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 11.8, loss: 0.81\n",
            "    Loss = 1.8650\n",
            "\n",
            "(w=2.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.0, loss: 1.21\n",
            "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
            "    Loss = 2.9250\n",
            "\n",
            "(w=2.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.2, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 2.6, loss: 0.74\n",
            "    x: 2.0, y: 5.0, y_hat: 4.4, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 7.7, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 8.8, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 11.0, loss: 0.01\n",
            "    Loss = 1.0248\n",
            "\n",
            "(w=2.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.8, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 4.6, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 7.9, loss: 0.00\n",
            "    x: 4.0, y: 9.1, y_hat: 9.0, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.2, loss: 0.09\n",
            "    Loss = 0.5928\n",
            "\n",
            "(w=2.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.1, loss: 0.04\n",
            "    x: 4.0, y: 9.1, y_hat: 9.2, loss: 0.01\n",
            "    x: 5.0, y: 10.9, y_hat: 11.4, loss: 0.25\n",
            "    Loss = 0.4008\n",
            "\n",
            "(w=2.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.2, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 8.3, loss: 0.16\n",
            "    x: 4.0, y: 9.1, y_hat: 9.4, loss: 0.09\n",
            "    x: 5.0, y: 10.9, y_hat: 11.6, loss: 0.49\n",
            "    Loss = 0.4488\n",
            "\n",
            "(w=2.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.5, loss: 0.36\n",
            "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 11.8, loss: 0.81\n",
            "    Loss = 0.7368\n",
            "\n",
            "(w=2.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.02\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 8.7, loss: 0.64\n",
            "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
            "    Loss = 1.2648\n",
            "\n",
            "(w=2.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.12\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 8.9, loss: 1.00\n",
            "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 12.2, loss: 1.69\n",
            "    Loss = 2.0328\n",
            "\n",
            "(w=2.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 9.1, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 10.2, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 12.4, loss: 2.25\n",
            "    Loss = 3.0408\n",
            "\n",
            "(w=2.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.3, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 12.6, loss: 2.89\n",
            "    Loss = 4.2888\n",
            "\n",
            "(w=2.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 9.5, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 12.8, loss: 3.61\n",
            "    Loss = 5.7768\n",
            "\n",
            "(w=2.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 9.7, loss: 3.24\n",
            "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
            "    Loss = 7.5048\n",
            "\n",
            "(w=2.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.4, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 2.9, loss: 0.38\n",
            "    x: 2.0, y: 5.0, y_hat: 4.8, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.4, loss: 0.25\n",
            "    x: 4.0, y: 9.1, y_hat: 9.6, loss: 0.25\n",
            "    x: 5.0, y: 10.9, y_hat: 12.0, loss: 1.21\n",
            "    Loss = 1.3122\n",
            "\n",
            "(w=2.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 5.0, loss: 0.00\n",
            "    x: 3.5, y: 7.9, y_hat: 8.6, loss: 0.49\n",
            "    x: 4.0, y: 9.1, y_hat: 9.8, loss: 0.49\n",
            "    x: 5.0, y: 10.9, y_hat: 12.2, loss: 1.69\n",
            "    Loss = 1.5482\n",
            "\n",
            "(w=2.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 8.8, loss: 0.81\n",
            "    x: 4.0, y: 9.1, y_hat: 10.0, loss: 0.81\n",
            "    x: 5.0, y: 10.9, y_hat: 12.4, loss: 2.25\n",
            "    Loss = 2.0242\n",
            "\n",
            "(w=2.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 9.0, loss: 1.21\n",
            "    x: 4.0, y: 9.1, y_hat: 10.2, loss: 1.21\n",
            "    x: 5.0, y: 10.9, y_hat: 12.6, loss: 2.89\n",
            "    Loss = 2.7402\n",
            "\n",
            "(w=2.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 9.2, loss: 1.69\n",
            "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 12.8, loss: 3.61\n",
            "    Loss = 3.6962\n",
            "\n",
            "(w=2.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.14\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 9.4, loss: 2.25\n",
            "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
            "    Loss = 4.8922\n",
            "\n",
            "(w=2.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.34\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.6, loss: 2.89\n",
            "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 13.2, loss: 5.29\n",
            "    Loss = 6.3282\n",
            "\n",
            "(w=2.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 9.8, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 11.0, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 13.4, loss: 6.25\n",
            "    Loss = 8.0042\n",
            "\n",
            "(w=2.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.0, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 13.6, loss: 7.29\n",
            "    Loss = 9.9202\n",
            "\n",
            "(w=2.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 10.2, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 13.8, loss: 8.41\n",
            "    Loss = 12.0762\n",
            "\n",
            "(w=2.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 10.4, loss: 6.25\n",
            "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
            "    Loss = 14.4722\n",
            "\n",
            "(w=2.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 3.1, loss: 0.14\n",
            "    x: 2.0, y: 5.0, y_hat: 5.2, loss: 0.04\n",
            "    x: 3.5, y: 7.9, y_hat: 9.1, loss: 1.44\n",
            "    x: 4.0, y: 9.1, y_hat: 10.4, loss: 1.69\n",
            "    x: 5.0, y: 10.9, y_hat: 13.0, loss: 4.41\n",
            "    Loss = 3.9872\n",
            "\n",
            "(w=2.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.3, loss: 0.03\n",
            "    x: 2.0, y: 5.0, y_hat: 5.4, loss: 0.16\n",
            "    x: 3.5, y: 7.9, y_hat: 9.3, loss: 1.96\n",
            "    x: 4.0, y: 9.1, y_hat: 10.6, loss: 2.25\n",
            "    x: 5.0, y: 10.9, y_hat: 13.2, loss: 5.29\n",
            "    Loss = 4.8912\n",
            "\n",
            "(w=2.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.5, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 9.5, loss: 2.56\n",
            "    x: 4.0, y: 9.1, y_hat: 10.8, loss: 2.89\n",
            "    x: 5.0, y: 10.9, y_hat: 13.4, loss: 6.25\n",
            "    Loss = 6.0352\n",
            "\n",
            "(w=2.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.7, loss: 0.05\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 9.7, loss: 3.24\n",
            "    x: 4.0, y: 9.1, y_hat: 11.0, loss: 3.61\n",
            "    x: 5.0, y: 10.9, y_hat: 13.6, loss: 7.29\n",
            "    Loss = 7.4192\n",
            "\n",
            "(w=2.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.9, loss: 0.18\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 9.9, loss: 4.00\n",
            "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 13.8, loss: 8.41\n",
            "    Loss = 9.0432\n",
            "\n",
            "(w=2.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.38\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 10.1, loss: 4.84\n",
            "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
            "    Loss = 10.9072\n",
            "\n",
            "(w=2.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.67\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.3, loss: 5.76\n",
            "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 14.2, loss: 10.89\n",
            "    Loss = 13.0112\n",
            "\n",
            "(w=2.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 10.5, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 11.8, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.4, loss: 12.25\n",
            "    Loss = 15.3552\n",
            "\n",
            "(w=2.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 10.7, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 14.6, loss: 13.69\n",
            "    Loss = 17.9392\n",
            "\n",
            "(w=2.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 10.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 14.8, loss: 15.21\n",
            "    Loss = 20.7632\n",
            "\n",
            "(w=2.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 11.1, loss: 10.24\n",
            "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
            "    Loss = 23.8272\n",
            "\n",
            "(w=2.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 2.8, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 3.4, loss: 0.02\n",
            "    x: 2.0, y: 5.0, y_hat: 5.6, loss: 0.36\n",
            "    x: 3.5, y: 7.9, y_hat: 9.8, loss: 3.61\n",
            "    x: 4.0, y: 9.1, y_hat: 11.2, loss: 4.41\n",
            "    x: 5.0, y: 10.9, y_hat: 14.0, loss: 9.61\n",
            "    Loss = 9.0498\n",
            "\n",
            "(w=2.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.00\n",
            "    x: 2.0, y: 5.0, y_hat: 5.8, loss: 0.64\n",
            "    x: 3.5, y: 7.9, y_hat: 10.0, loss: 4.41\n",
            "    x: 4.0, y: 9.1, y_hat: 11.4, loss: 5.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.2, loss: 10.89\n",
            "    Loss = 10.6218\n",
            "\n",
            "(w=2.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.07\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 10.2, loss: 5.29\n",
            "    x: 4.0, y: 9.1, y_hat: 11.6, loss: 6.25\n",
            "    x: 5.0, y: 10.9, y_hat: 14.4, loss: 12.25\n",
            "    Loss = 12.4338\n",
            "\n",
            "(w=2.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.21\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 10.4, loss: 6.25\n",
            "    x: 4.0, y: 9.1, y_hat: 11.8, loss: 7.29\n",
            "    x: 5.0, y: 10.9, y_hat: 14.6, loss: 13.69\n",
            "    Loss = 14.4858\n",
            "\n",
            "(w=2.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.44\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.6, loss: 7.29\n",
            "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 14.8, loss: 15.21\n",
            "    Loss = 16.7778\n",
            "\n",
            "(w=2.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.74\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 10.8, loss: 8.41\n",
            "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
            "    Loss = 19.3098\n",
            "\n",
            "(w=2.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.12\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.0, loss: 9.61\n",
            "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 15.2, loss: 18.49\n",
            "    Loss = 22.0818\n",
            "\n",
            "(w=2.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 11.2, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 12.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 15.4, loss: 20.25\n",
            "    Loss = 25.0938\n",
            "\n",
            "(w=2.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 11.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 15.6, loss: 22.09\n",
            "    Loss = 28.3458\n",
            "\n",
            "(w=2.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 11.6, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 15.8, loss: 24.01\n",
            "    Loss = 31.8378\n",
            "\n",
            "(w=2.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 11.8, loss: 15.21\n",
            "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
            "    Loss = 35.5698\n",
            "\n",
            "(w=3.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.0, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.6, loss: 0.01\n",
            "    x: 2.0, y: 5.0, y_hat: 6.0, loss: 1.00\n",
            "    x: 3.5, y: 7.9, y_hat: 10.5, loss: 6.76\n",
            "    x: 4.0, y: 9.1, y_hat: 12.0, loss: 8.41\n",
            "    x: 5.0, y: 10.9, y_hat: 15.0, loss: 16.81\n",
            "    Loss = 16.5000\n",
            "\n",
            "(w=3.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.09\n",
            "    x: 2.0, y: 5.0, y_hat: 6.2, loss: 1.44\n",
            "    x: 3.5, y: 7.9, y_hat: 10.7, loss: 7.84\n",
            "    x: 4.0, y: 9.1, y_hat: 12.2, loss: 9.61\n",
            "    x: 5.0, y: 10.9, y_hat: 15.2, loss: 18.49\n",
            "    Loss = 18.7400\n",
            "\n",
            "(w=3.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.25\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 10.9, loss: 9.00\n",
            "    x: 4.0, y: 9.1, y_hat: 12.4, loss: 10.89\n",
            "    x: 5.0, y: 10.9, y_hat: 15.4, loss: 20.25\n",
            "    Loss = 21.2200\n",
            "\n",
            "(w=3.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.49\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 11.1, loss: 10.24\n",
            "    x: 4.0, y: 9.1, y_hat: 12.6, loss: 12.25\n",
            "    x: 5.0, y: 10.9, y_hat: 15.6, loss: 22.09\n",
            "    Loss = 23.9400\n",
            "\n",
            "(w=3.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.81\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.3, loss: 11.56\n",
            "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 15.8, loss: 24.01\n",
            "    Loss = 26.9000\n",
            "\n",
            "(w=3.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.21\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 11.5, loss: 12.96\n",
            "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
            "    Loss = 30.1000\n",
            "\n",
            "(w=3.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.69\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 11.7, loss: 14.44\n",
            "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 16.2, loss: 28.09\n",
            "    Loss = 33.5400\n",
            "\n",
            "(w=3.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 11.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 13.4, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 16.4, loss: 30.25\n",
            "    Loss = 37.2200\n",
            "\n",
            "(w=3.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.1, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 16.6, loss: 32.49\n",
            "    Loss = 41.1400\n",
            "\n",
            "(w=3.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.3, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 16.8, loss: 34.81\n",
            "    Loss = 45.3000\n",
            "\n",
            "(w=3.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 12.5, loss: 21.16\n",
            "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
            "    Loss = 49.7000\n",
            "\n",
            "(w=3.2, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.2, loss: 0.01\n",
            "    x: 1.2, y: 3.5, y_hat: 3.8, loss: 0.12\n",
            "    x: 2.0, y: 5.0, y_hat: 6.4, loss: 1.96\n",
            "    x: 3.5, y: 7.9, y_hat: 11.2, loss: 10.89\n",
            "    x: 4.0, y: 9.1, y_hat: 12.8, loss: 13.69\n",
            "    x: 5.0, y: 10.9, y_hat: 16.0, loss: 26.01\n",
            "    Loss = 26.3378\n",
            "\n",
            "(w=3.2, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.0, loss: 0.29\n",
            "    x: 2.0, y: 5.0, y_hat: 6.6, loss: 2.56\n",
            "    x: 3.5, y: 7.9, y_hat: 11.4, loss: 12.25\n",
            "    x: 4.0, y: 9.1, y_hat: 13.0, loss: 15.21\n",
            "    x: 5.0, y: 10.9, y_hat: 16.2, loss: 28.09\n",
            "    Loss = 29.2458\n",
            "\n",
            "(w=3.2, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.2, loss: 0.55\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.6, loss: 13.69\n",
            "    x: 4.0, y: 9.1, y_hat: 13.2, loss: 16.81\n",
            "    x: 5.0, y: 10.9, y_hat: 16.4, loss: 30.25\n",
            "    Loss = 32.3938\n",
            "\n",
            "(w=3.2, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.4, loss: 0.88\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 11.8, loss: 15.21\n",
            "    x: 4.0, y: 9.1, y_hat: 13.4, loss: 18.49\n",
            "    x: 5.0, y: 10.9, y_hat: 16.6, loss: 32.49\n",
            "    Loss = 35.7818\n",
            "\n",
            "(w=3.2, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.30\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.0, loss: 16.81\n",
            "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 16.8, loss: 34.81\n",
            "    Loss = 39.4098\n",
            "\n",
            "(w=3.2, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.80\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.2, loss: 18.49\n",
            "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
            "    Loss = 43.2778\n",
            "\n",
            "(w=3.2, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.37\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.4, loss: 20.25\n",
            "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.2, loss: 39.69\n",
            "    Loss = 47.3858\n",
            "\n",
            "(w=3.2, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 3.03\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.6, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 14.2, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.4, loss: 42.25\n",
            "    Loss = 51.7338\n",
            "\n",
            "(w=3.2, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.76\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 12.8, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.6, loss: 44.89\n",
            "    Loss = 56.3218\n",
            "\n",
            "(w=3.2, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.58\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.0, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 17.8, loss: 47.61\n",
            "    Loss = 61.1498\n",
            "\n",
            "(w=3.2, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.48\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 13.2, loss: 28.09\n",
            "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
            "    Loss = 66.2178\n",
            "\n",
            "(w=3.4, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.4, loss: 0.09\n",
            "    x: 1.2, y: 3.5, y_hat: 4.1, loss: 0.34\n",
            "    x: 2.0, y: 5.0, y_hat: 6.8, loss: 3.24\n",
            "    x: 3.5, y: 7.9, y_hat: 11.9, loss: 16.00\n",
            "    x: 4.0, y: 9.1, y_hat: 13.6, loss: 20.25\n",
            "    x: 5.0, y: 10.9, y_hat: 17.0, loss: 37.21\n",
            "    Loss = 38.5632\n",
            "\n",
            "(w=3.4, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.61\n",
            "    x: 2.0, y: 5.0, y_hat: 7.0, loss: 4.00\n",
            "    x: 3.5, y: 7.9, y_hat: 12.1, loss: 17.64\n",
            "    x: 4.0, y: 9.1, y_hat: 13.8, loss: 22.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.2, loss: 39.69\n",
            "    Loss = 42.1392\n",
            "\n",
            "(w=3.4, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 0.96\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.3, loss: 19.36\n",
            "    x: 4.0, y: 9.1, y_hat: 14.0, loss: 24.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.4, loss: 42.25\n",
            "    Loss = 45.9552\n",
            "\n",
            "(w=3.4, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.39\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.5, loss: 21.16\n",
            "    x: 4.0, y: 9.1, y_hat: 14.2, loss: 26.01\n",
            "    x: 5.0, y: 10.9, y_hat: 17.6, loss: 44.89\n",
            "    Loss = 50.0112\n",
            "\n",
            "(w=3.4, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 1.90\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.7, loss: 23.04\n",
            "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 17.8, loss: 47.61\n",
            "    Loss = 54.3072\n",
            "\n",
            "(w=3.4, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.50\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.9, loss: 25.00\n",
            "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
            "    Loss = 58.8432\n",
            "\n",
            "(w=3.4, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.3, loss: 3.17\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 13.1, loss: 27.04\n",
            "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 18.2, loss: 53.29\n",
            "    Loss = 63.6192\n",
            "\n",
            "(w=3.4, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.5, loss: 3.92\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.3, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 15.0, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 18.4, loss: 56.25\n",
            "    Loss = 68.6352\n",
            "\n",
            "(w=3.4, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.7, loss: 4.75\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 13.5, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 18.6, loss: 59.29\n",
            "    Loss = 73.8912\n",
            "\n",
            "(w=3.4, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 5.9, loss: 5.66\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 13.7, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 18.8, loss: 62.41\n",
            "    Loss = 79.3872\n",
            "\n",
            "(w=3.4, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.1, loss: 6.66\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 13.9, loss: 36.00\n",
            "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
            "    Loss = 85.1232\n",
            "\n",
            "(w=3.6, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.6, loss: 0.25\n",
            "    x: 1.2, y: 3.5, y_hat: 4.3, loss: 0.67\n",
            "    x: 2.0, y: 5.0, y_hat: 7.2, loss: 4.84\n",
            "    x: 3.5, y: 7.9, y_hat: 12.6, loss: 22.09\n",
            "    x: 4.0, y: 9.1, y_hat: 14.4, loss: 28.09\n",
            "    x: 5.0, y: 10.9, y_hat: 18.0, loss: 50.41\n",
            "    Loss = 53.1762\n",
            "\n",
            "(w=3.6, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.5, loss: 1.04\n",
            "    x: 2.0, y: 5.0, y_hat: 7.4, loss: 5.76\n",
            "    x: 3.5, y: 7.9, y_hat: 12.8, loss: 24.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.6, loss: 30.25\n",
            "    x: 5.0, y: 10.9, y_hat: 18.2, loss: 53.29\n",
            "    Loss = 57.4202\n",
            "\n",
            "(w=3.6, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.7, loss: 1.49\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 13.0, loss: 26.01\n",
            "    x: 4.0, y: 9.1, y_hat: 14.8, loss: 32.49\n",
            "    x: 5.0, y: 10.9, y_hat: 18.4, loss: 56.25\n",
            "    Loss = 61.9042\n",
            "\n",
            "(w=3.6, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 4.9, loss: 2.02\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 13.2, loss: 28.09\n",
            "    x: 4.0, y: 9.1, y_hat: 15.0, loss: 34.81\n",
            "    x: 5.0, y: 10.9, y_hat: 18.6, loss: 59.29\n",
            "    Loss = 66.6282\n",
            "\n",
            "(w=3.6, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.1, loss: 2.62\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 13.4, loss: 30.25\n",
            "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 18.8, loss: 62.41\n",
            "    Loss = 71.5922\n",
            "\n",
            "(w=3.6, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.3, loss: 3.31\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.6, loss: 32.49\n",
            "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
            "    Loss = 76.7962\n",
            "\n",
            "(w=3.6, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.5, loss: 4.08\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 13.8, loss: 34.81\n",
            "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 19.2, loss: 68.89\n",
            "    Loss = 82.2402\n",
            "\n",
            "(w=3.6, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.7, loss: 4.93\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 14.0, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 15.8, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 19.4, loss: 72.25\n",
            "    Loss = 87.9242\n",
            "\n",
            "(w=3.6, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 5.9, loss: 5.86\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 14.2, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 19.6, loss: 75.69\n",
            "    Loss = 93.8482\n",
            "\n",
            "(w=3.6, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.1, loss: 6.86\n",
            "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 14.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 19.8, loss: 79.21\n",
            "    Loss = 100.0122\n",
            "\n",
            "(w=3.6, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 6.3, loss: 7.95\n",
            "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 14.6, loss: 44.89\n",
            "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
            "    Loss = 106.4162\n",
            "\n",
            "(w=3.8, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 3.8, loss: 0.49\n",
            "    x: 1.2, y: 3.5, y_hat: 4.6, loss: 1.12\n",
            "    x: 2.0, y: 5.0, y_hat: 7.6, loss: 6.76\n",
            "    x: 3.5, y: 7.9, y_hat: 13.3, loss: 29.16\n",
            "    x: 4.0, y: 9.1, y_hat: 15.2, loss: 37.21\n",
            "    x: 5.0, y: 10.9, y_hat: 19.0, loss: 65.61\n",
            "    Loss = 70.1768\n",
            "\n",
            "(w=3.8, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.59\n",
            "    x: 2.0, y: 5.0, y_hat: 7.8, loss: 7.84\n",
            "    x: 3.5, y: 7.9, y_hat: 13.5, loss: 31.36\n",
            "    x: 4.0, y: 9.1, y_hat: 15.4, loss: 39.69\n",
            "    x: 5.0, y: 10.9, y_hat: 19.2, loss: 68.89\n",
            "    Loss = 75.0888\n",
            "\n",
            "(w=3.8, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.13\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 13.7, loss: 33.64\n",
            "    x: 4.0, y: 9.1, y_hat: 15.6, loss: 42.25\n",
            "    x: 5.0, y: 10.9, y_hat: 19.4, loss: 72.25\n",
            "    Loss = 80.2408\n",
            "\n",
            "(w=3.8, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.76\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 13.9, loss: 36.00\n",
            "    x: 4.0, y: 9.1, y_hat: 15.8, loss: 44.89\n",
            "    x: 5.0, y: 10.9, y_hat: 19.6, loss: 75.69\n",
            "    Loss = 85.6328\n",
            "\n",
            "(w=3.8, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.46\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 14.1, loss: 38.44\n",
            "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 19.8, loss: 79.21\n",
            "    Loss = 91.2648\n",
            "\n",
            "(w=3.8, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.24\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 14.3, loss: 40.96\n",
            "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
            "    Loss = 97.1368\n",
            "\n",
            "(w=3.8, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.11\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 14.5, loss: 43.56\n",
            "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.2, loss: 86.49\n",
            "    Loss = 103.2488\n",
            "\n",
            "(w=3.8, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 6.0, loss: 6.05\n",
            "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 14.7, loss: 46.24\n",
            "    x: 4.0, y: 9.1, y_hat: 16.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 20.4, loss: 90.25\n",
            "    Loss = 109.6008\n",
            "\n",
            "(w=3.8, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.2, loss: 7.08\n",
            "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 14.9, loss: 49.00\n",
            "    x: 4.0, y: 9.1, y_hat: 16.8, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.6, loss: 94.09\n",
            "    Loss = 116.1928\n",
            "\n",
            "(w=3.8, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 6.4, loss: 8.18\n",
            "    x: 2.0, y: 5.0, y_hat: 9.4, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 15.1, loss: 51.84\n",
            "    x: 4.0, y: 9.1, y_hat: 17.0, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 20.8, loss: 98.01\n",
            "    Loss = 123.0248\n",
            "\n",
            "(w=3.8, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.8, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.6, loss: 9.36\n",
            "    x: 2.0, y: 5.0, y_hat: 9.6, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 15.3, loss: 54.76\n",
            "    x: 4.0, y: 9.1, y_hat: 17.2, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 21.0, loss: 102.01\n",
            "    Loss = 130.0968\n",
            "\n",
            "(w=4.0, b=0.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.0, loss: 0.81\n",
            "    x: 1.2, y: 3.5, y_hat: 4.8, loss: 1.69\n",
            "    x: 2.0, y: 5.0, y_hat: 8.0, loss: 9.00\n",
            "    x: 3.5, y: 7.9, y_hat: 14.0, loss: 37.21\n",
            "    x: 4.0, y: 9.1, y_hat: 16.0, loss: 47.61\n",
            "    x: 5.0, y: 10.9, y_hat: 20.0, loss: 82.81\n",
            "    Loss = 89.5650\n",
            "\n",
            "(w=4.0, b=0.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.2, loss: 1.21\n",
            "    x: 1.2, y: 3.5, y_hat: 5.0, loss: 2.25\n",
            "    x: 2.0, y: 5.0, y_hat: 8.2, loss: 10.24\n",
            "    x: 3.5, y: 7.9, y_hat: 14.2, loss: 39.69\n",
            "    x: 4.0, y: 9.1, y_hat: 16.2, loss: 50.41\n",
            "    x: 5.0, y: 10.9, y_hat: 20.2, loss: 86.49\n",
            "    Loss = 95.1450\n",
            "\n",
            "(w=4.0, b=0.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.4, loss: 1.69\n",
            "    x: 1.2, y: 3.5, y_hat: 5.2, loss: 2.89\n",
            "    x: 2.0, y: 5.0, y_hat: 8.4, loss: 11.56\n",
            "    x: 3.5, y: 7.9, y_hat: 14.4, loss: 42.25\n",
            "    x: 4.0, y: 9.1, y_hat: 16.4, loss: 53.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.4, loss: 90.25\n",
            "    Loss = 100.9650\n",
            "\n",
            "(w=4.0, b=0.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.6, loss: 2.25\n",
            "    x: 1.2, y: 3.5, y_hat: 5.4, loss: 3.61\n",
            "    x: 2.0, y: 5.0, y_hat: 8.6, loss: 12.96\n",
            "    x: 3.5, y: 7.9, y_hat: 14.6, loss: 44.89\n",
            "    x: 4.0, y: 9.1, y_hat: 16.6, loss: 56.25\n",
            "    x: 5.0, y: 10.9, y_hat: 20.6, loss: 94.09\n",
            "    Loss = 107.0250\n",
            "\n",
            "(w=4.0, b=0.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 4.8, loss: 2.89\n",
            "    x: 1.2, y: 3.5, y_hat: 5.6, loss: 4.41\n",
            "    x: 2.0, y: 5.0, y_hat: 8.8, loss: 14.44\n",
            "    x: 3.5, y: 7.9, y_hat: 14.8, loss: 47.61\n",
            "    x: 4.0, y: 9.1, y_hat: 16.8, loss: 59.29\n",
            "    x: 5.0, y: 10.9, y_hat: 20.8, loss: 98.01\n",
            "    Loss = 113.3250\n",
            "\n",
            "(w=4.0, b=1.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.0, loss: 3.61\n",
            "    x: 1.2, y: 3.5, y_hat: 5.8, loss: 5.29\n",
            "    x: 2.0, y: 5.0, y_hat: 9.0, loss: 16.00\n",
            "    x: 3.5, y: 7.9, y_hat: 15.0, loss: 50.41\n",
            "    x: 4.0, y: 9.1, y_hat: 17.0, loss: 62.41\n",
            "    x: 5.0, y: 10.9, y_hat: 21.0, loss: 102.01\n",
            "    Loss = 119.8650\n",
            "\n",
            "(w=4.0, b=1.2)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.2, loss: 4.41\n",
            "    x: 1.2, y: 3.5, y_hat: 6.0, loss: 6.25\n",
            "    x: 2.0, y: 5.0, y_hat: 9.2, loss: 17.64\n",
            "    x: 3.5, y: 7.9, y_hat: 15.2, loss: 53.29\n",
            "    x: 4.0, y: 9.1, y_hat: 17.2, loss: 65.61\n",
            "    x: 5.0, y: 10.9, y_hat: 21.2, loss: 106.09\n",
            "    Loss = 126.6450\n",
            "\n",
            "(w=4.0, b=1.4)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.4, loss: 5.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.2, loss: 7.29\n",
            "    x: 2.0, y: 5.0, y_hat: 9.4, loss: 19.36\n",
            "    x: 3.5, y: 7.9, y_hat: 15.4, loss: 56.25\n",
            "    x: 4.0, y: 9.1, y_hat: 17.4, loss: 68.89\n",
            "    x: 5.0, y: 10.9, y_hat: 21.4, loss: 110.25\n",
            "    Loss = 133.6650\n",
            "\n",
            "(w=4.0, b=1.6)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.6, loss: 6.25\n",
            "    x: 1.2, y: 3.5, y_hat: 6.4, loss: 8.41\n",
            "    x: 2.0, y: 5.0, y_hat: 9.6, loss: 21.16\n",
            "    x: 3.5, y: 7.9, y_hat: 15.6, loss: 59.29\n",
            "    x: 4.0, y: 9.1, y_hat: 17.6, loss: 72.25\n",
            "    x: 5.0, y: 10.9, y_hat: 21.6, loss: 114.49\n",
            "    Loss = 140.9250\n",
            "\n",
            "(w=4.0, b=1.8)\n",
            "    x: 1.0, y: 3.1, y_hat: 5.8, loss: 7.29\n",
            "    x: 1.2, y: 3.5, y_hat: 6.6, loss: 9.61\n",
            "    x: 2.0, y: 5.0, y_hat: 9.8, loss: 23.04\n",
            "    x: 3.5, y: 7.9, y_hat: 15.8, loss: 62.41\n",
            "    x: 4.0, y: 9.1, y_hat: 17.8, loss: 75.69\n",
            "    x: 5.0, y: 10.9, y_hat: 21.8, loss: 118.81\n",
            "    Loss = 148.4250\n",
            "\n",
            "(w=4.0, b=2.0)\n",
            "    x: 1.0, y: 3.1, y_hat: 6.0, loss: 8.41\n",
            "    x: 1.2, y: 3.5, y_hat: 6.8, loss: 10.89\n",
            "    x: 2.0, y: 5.0, y_hat: 10.0, loss: 25.00\n",
            "    x: 3.5, y: 7.9, y_hat: 16.0, loss: 65.61\n",
            "    x: 4.0, y: 9.1, y_hat: 18.0, loss: 79.21\n",
            "    x: 5.0, y: 10.9, y_hat: 22.0, loss: 123.21\n",
            "    Loss = 156.1650\n",
            "\n",
            "BEST:\n",
            "w=2.0, b=1.0, loss=0.0250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdvNc6WQOwhv"
      },
      "source": [
        "Plotting the loss values as a surface graph gives you a picture of the \"optimisation landscape\" for the parameter values. The loss is minimum at $w=2$, $b=1$ (it might be hard to see this clearly from the 3D diagram, but you can trust the numbers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVruAU86OwIA",
        "outputId": "7e1ee489-660e-4d41-be10-835db5ebfb81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "source": [
        "fig = plt.figure()\n",
        "\n",
        "# enable 3D\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        " # generate combinations of weights and biases\n",
        "(w_list, b_list) = np.meshgrid(weights, biases)\n",
        "\n",
        "# plot loss across weights and bias values\n",
        "surf = ax.plot_surface(w_list.T, b_list.T, loss_matrix,\n",
        "                       linewidth=0, antialiased=False)\n",
        "\n",
        "ax.set_xlabel('w')\n",
        "ax.set_ylabel('b')\n",
        "ax.set_zlabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nWYSrkBcWzs"
      },
      "source": [
        "### Optimisation with derivatives\n",
        "\n",
        "Obviously, the brute force search method above is really computationally expensive (very slow!), especially when there are more parameters and more values to search. A more efficient approach is to start from a random set of parameter values, and then try to \"move down\" the loss graph to the point with a minimum loss. So, in the plot above, you can start at some random $w$ and $b$, and try to move towards the minimum point.\n",
        "\n",
        "The problem is: from where you are, you actually do not know where the minimum point is. You only know that there *is* a minimum point. The question is: how do you know towards which direction you should move (and how far) so that you can get to the minimum point as fast as possible? The answer is downwards (obviously!) and where the slope is steepest.\n",
        "\n",
        "Fortunately, calculus saves us from guessing, and provides us a way to compute the direction of the slope at any point. This is called the *derivative* (or gradient). Intuitively, the derivative $\\frac{\\partial L}{\\partial w}$ tells us by how much $L$ changes when $w$ changes. Similarly, $\\frac{\\partial L}{\\partial b}$ tells us by how much $L$ changes when $b$ changes. So if we move in these directions, we hope that we can ultimately reach a minimum point.\n",
        "\n",
        "If you are well-versed with calculus, you can compute $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ by hand. Otherwise, you can get help from a derivative calculator (e.g. https://www.derivative-calculator.net)\n",
        "\n",
        "As presented in the lectures, the partial derivatives of the loss function with respect to $w$ and to $b$ are:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x^{(i)}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^{N} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$\n",
        "\n",
        "Thus, for a *single* point, the partial derivatives are:\n",
        "\n",
        "$\\frac{\\partial L^{(i)}}{\\partial w} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)x^{(i)}$\n",
        "\n",
        "$\\frac{\\partial L^{(i)}}{\\partial b} = \\left(\\hat{y}^{(i)} - y^{(i)}\\right)$\n",
        "\n",
        "Now, complete the `gradient()` method for `SimpleLinearRegression` below to compute the partial derivatives wrt $w$ and $b$ at a given point. To make life easier, just compute and return both $\\frac{\\partial L^{(i)}}{\\partial w}$ and $\\frac{\\partial L^{(i)}}{\\partial b}$ at the same time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBXmI71Ewc9e",
        "outputId": "56e269f4-378a-4406-f86d-c04b8604fb86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def gradient(self, x, y):\n",
        "    \"\"\" Compute partial derivatives wrt w and b\n",
        "\n",
        "    Args:\n",
        "        x (float): input instance\n",
        "        y (float): ground truth output\n",
        "\n",
        "    Returns:\n",
        "        tuple: (float, float)\n",
        "            - the first element will be dL/dw\n",
        "            - the second element will be dL/db\n",
        "    \"\"\"\n",
        "    # TODO: Complete this\n",
        "    y_hat = self.forward(x)\n",
        "    return ((y_hat - y)*x, (y_hat - y))\n",
        "\n",
        "\n",
        "# A quick hack to bind this function as the SimpleLinearRegression.gradient() method\n",
        "SimpleLinearRegression.gradient = gradient\n",
        "\n",
        "\n",
        "## Quick test: This should return (6.0, 3.0)\n",
        "model = SimpleLinearRegression()\n",
        "model.w = 3\n",
        "model.b = 2\n",
        "x = 2.0\n",
        "y = 5.0\n",
        "(dLdw, dLdb) = model.gradient(x, y)\n",
        "print(dLdw) # should print 6.0\n",
        "print(dLdb) # should print 3.0"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n",
            "3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBw1pRVN-heL"
      },
      "source": [
        "So, in this example, $\\frac{\\partial L}{\\partial w}=6.0$ suggests that when $w$ increases by a very tiny amount, $L$ will increase by 6.0 times that amount.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCNctFEY0yiT"
      },
      "source": [
        "#### Gradient descent\n",
        "\n",
        "Now that we have our gradients, let us try to optimise the parameters $w$ and $b$ of our model to minimise the loss. We will use the gradient descent algorithm for this, as discussed in the lectures.\n",
        "\n",
        "You may reimplement the code provided in the lectures, replacing some of the code by reusing the `forward()`, `loss()` and `gradient()` methods of `SimpleLinearRegression` that you implemented earlier. This will help make your code more modular and readable, and help to improve your understanding of gradient descent at a more abstract level.\n",
        "\n",
        "You should be able to obtain $w \\approx 2$ and $b \\approx 1$ by the end of training if you have implemented everything correctly. Also experiment with the learning rate and the number of epochs and observe the effects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLdqp4ESCpsR",
        "outputId": "1555643e-4249-4fba-c92d-10e8776b444e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    error = 0.0\n",
        "    grad_w = 0.0\n",
        "    grad_b = 0.0\n",
        "    for (x, y) in zip(x_train, y_train):\n",
        "        ### TODO: Complete this\n",
        "        ### 1. Compute the gradients for w and b for this example\n",
        "        (dLdw, dLdb) = model.gradient(x, y)\n",
        "\n",
        "        ### 2. Add the gradients to grad_w and grad_b\n",
        "        grad_w += dLdw\n",
        "        grad_b += dLdb\n",
        "\n",
        "        ### 3. Add the \"local\" loss to the global error (Loss) for analysis\n",
        "        error += model.loss(x,y)\n",
        "\n",
        "    # TODO: Update the weights using the (summed) gradients\n",
        "    model.w = model.w - learning_rate*grad_w\n",
        "    model.b = model.b - learning_rate*grad_b\n",
        "\n",
        "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\t w: 1.48\t b: 0.35\t L: 236.7588\n",
            "Epoch: 1\t w: 1.90\t b: 0.47\t L: 30.3327\n",
            "Epoch: 2\t w: 2.04\t b: 0.52\t L: 4.2502\n",
            "Epoch: 3\t w: 2.09\t b: 0.55\t L: 0.9462\n",
            "Epoch: 4\t w: 2.11\t b: 0.56\t L: 0.5195\n",
            "Epoch: 5\t w: 2.12\t b: 0.57\t L: 0.4565\n",
            "Epoch: 6\t w: 2.12\t b: 0.58\t L: 0.4396\n",
            "Epoch: 7\t w: 2.12\t b: 0.58\t L: 0.4287\n",
            "Epoch: 8\t w: 2.11\t b: 0.59\t L: 0.4188\n",
            "Epoch: 9\t w: 2.11\t b: 0.60\t L: 0.4093\n",
            "Epoch: 10\t w: 2.11\t b: 0.60\t L: 0.4000\n",
            "Epoch: 11\t w: 2.11\t b: 0.61\t L: 0.3909\n",
            "Epoch: 12\t w: 2.11\t b: 0.61\t L: 0.3821\n",
            "Epoch: 13\t w: 2.11\t b: 0.62\t L: 0.3734\n",
            "Epoch: 14\t w: 2.10\t b: 0.63\t L: 0.3650\n",
            "Epoch: 15\t w: 2.10\t b: 0.63\t L: 0.3568\n",
            "Epoch: 16\t w: 2.10\t b: 0.64\t L: 0.3488\n",
            "Epoch: 17\t w: 2.10\t b: 0.65\t L: 0.3410\n",
            "Epoch: 18\t w: 2.10\t b: 0.65\t L: 0.3333\n",
            "Epoch: 19\t w: 2.09\t b: 0.66\t L: 0.3259\n",
            "Epoch: 20\t w: 2.09\t b: 0.66\t L: 0.3186\n",
            "Epoch: 21\t w: 2.09\t b: 0.67\t L: 0.3115\n",
            "Epoch: 22\t w: 2.09\t b: 0.67\t L: 0.3046\n",
            "Epoch: 23\t w: 2.09\t b: 0.68\t L: 0.2979\n",
            "Epoch: 24\t w: 2.09\t b: 0.68\t L: 0.2913\n",
            "Epoch: 25\t w: 2.09\t b: 0.69\t L: 0.2849\n",
            "Epoch: 26\t w: 2.08\t b: 0.70\t L: 0.2786\n",
            "Epoch: 27\t w: 2.08\t b: 0.70\t L: 0.2725\n",
            "Epoch: 28\t w: 2.08\t b: 0.71\t L: 0.2666\n",
            "Epoch: 29\t w: 2.08\t b: 0.71\t L: 0.2608\n",
            "Epoch: 30\t w: 2.08\t b: 0.72\t L: 0.2551\n",
            "Epoch: 31\t w: 2.08\t b: 0.72\t L: 0.2496\n",
            "Epoch: 32\t w: 2.08\t b: 0.73\t L: 0.2442\n",
            "Epoch: 33\t w: 2.07\t b: 0.73\t L: 0.2389\n",
            "Epoch: 34\t w: 2.07\t b: 0.74\t L: 0.2338\n",
            "Epoch: 35\t w: 2.07\t b: 0.74\t L: 0.2288\n",
            "Epoch: 36\t w: 2.07\t b: 0.75\t L: 0.2239\n",
            "Epoch: 37\t w: 2.07\t b: 0.75\t L: 0.2191\n",
            "Epoch: 38\t w: 2.07\t b: 0.75\t L: 0.2145\n",
            "Epoch: 39\t w: 2.07\t b: 0.76\t L: 0.2099\n",
            "Epoch: 40\t w: 2.06\t b: 0.76\t L: 0.2055\n",
            "Epoch: 41\t w: 2.06\t b: 0.77\t L: 0.2012\n",
            "Epoch: 42\t w: 2.06\t b: 0.77\t L: 0.1970\n",
            "Epoch: 43\t w: 2.06\t b: 0.78\t L: 0.1928\n",
            "Epoch: 44\t w: 2.06\t b: 0.78\t L: 0.1888\n",
            "Epoch: 45\t w: 2.06\t b: 0.79\t L: 0.1849\n",
            "Epoch: 46\t w: 2.06\t b: 0.79\t L: 0.1811\n",
            "Epoch: 47\t w: 2.06\t b: 0.79\t L: 0.1774\n",
            "Epoch: 48\t w: 2.05\t b: 0.80\t L: 0.1738\n",
            "Epoch: 49\t w: 2.05\t b: 0.80\t L: 0.1702\n",
            "Epoch: 50\t w: 2.05\t b: 0.81\t L: 0.1668\n",
            "Epoch: 51\t w: 2.05\t b: 0.81\t L: 0.1634\n",
            "Epoch: 52\t w: 2.05\t b: 0.81\t L: 0.1601\n",
            "Epoch: 53\t w: 2.05\t b: 0.82\t L: 0.1569\n",
            "Epoch: 54\t w: 2.05\t b: 0.82\t L: 0.1538\n",
            "Epoch: 55\t w: 2.05\t b: 0.82\t L: 0.1507\n",
            "Epoch: 56\t w: 2.05\t b: 0.83\t L: 0.1477\n",
            "Epoch: 57\t w: 2.04\t b: 0.83\t L: 0.1448\n",
            "Epoch: 58\t w: 2.04\t b: 0.84\t L: 0.1420\n",
            "Epoch: 59\t w: 2.04\t b: 0.84\t L: 0.1392\n",
            "Epoch: 60\t w: 2.04\t b: 0.84\t L: 0.1365\n",
            "Epoch: 61\t w: 2.04\t b: 0.85\t L: 0.1339\n",
            "Epoch: 62\t w: 2.04\t b: 0.85\t L: 0.1313\n",
            "Epoch: 63\t w: 2.04\t b: 0.85\t L: 0.1288\n",
            "Epoch: 64\t w: 2.04\t b: 0.86\t L: 0.1264\n",
            "Epoch: 65\t w: 2.04\t b: 0.86\t L: 0.1240\n",
            "Epoch: 66\t w: 2.04\t b: 0.86\t L: 0.1216\n",
            "Epoch: 67\t w: 2.04\t b: 0.87\t L: 0.1194\n",
            "Epoch: 68\t w: 2.03\t b: 0.87\t L: 0.1172\n",
            "Epoch: 69\t w: 2.03\t b: 0.87\t L: 0.1150\n",
            "Epoch: 70\t w: 2.03\t b: 0.88\t L: 0.1129\n",
            "Epoch: 71\t w: 2.03\t b: 0.88\t L: 0.1108\n",
            "Epoch: 72\t w: 2.03\t b: 0.88\t L: 0.1088\n",
            "Epoch: 73\t w: 2.03\t b: 0.88\t L: 0.1069\n",
            "Epoch: 74\t w: 2.03\t b: 0.89\t L: 0.1050\n",
            "Epoch: 75\t w: 2.03\t b: 0.89\t L: 0.1031\n",
            "Epoch: 76\t w: 2.03\t b: 0.89\t L: 0.1013\n",
            "Epoch: 77\t w: 2.03\t b: 0.90\t L: 0.0995\n",
            "Epoch: 78\t w: 2.03\t b: 0.90\t L: 0.0978\n",
            "Epoch: 79\t w: 2.03\t b: 0.90\t L: 0.0961\n",
            "Epoch: 80\t w: 2.02\t b: 0.90\t L: 0.0945\n",
            "Epoch: 81\t w: 2.02\t b: 0.91\t L: 0.0929\n",
            "Epoch: 82\t w: 2.02\t b: 0.91\t L: 0.0913\n",
            "Epoch: 83\t w: 2.02\t b: 0.91\t L: 0.0898\n",
            "Epoch: 84\t w: 2.02\t b: 0.91\t L: 0.0883\n",
            "Epoch: 85\t w: 2.02\t b: 0.92\t L: 0.0868\n",
            "Epoch: 86\t w: 2.02\t b: 0.92\t L: 0.0854\n",
            "Epoch: 87\t w: 2.02\t b: 0.92\t L: 0.0840\n",
            "Epoch: 88\t w: 2.02\t b: 0.92\t L: 0.0827\n",
            "Epoch: 89\t w: 2.02\t b: 0.93\t L: 0.0813\n",
            "Epoch: 90\t w: 2.02\t b: 0.93\t L: 0.0801\n",
            "Epoch: 91\t w: 2.02\t b: 0.93\t L: 0.0788\n",
            "Epoch: 92\t w: 2.02\t b: 0.93\t L: 0.0776\n",
            "Epoch: 93\t w: 2.01\t b: 0.94\t L: 0.0764\n",
            "Epoch: 94\t w: 2.01\t b: 0.94\t L: 0.0752\n",
            "Epoch: 95\t w: 2.01\t b: 0.94\t L: 0.0741\n",
            "Epoch: 96\t w: 2.01\t b: 0.94\t L: 0.0730\n",
            "Epoch: 97\t w: 2.01\t b: 0.95\t L: 0.0719\n",
            "Epoch: 98\t w: 2.01\t b: 0.95\t L: 0.0709\n",
            "Epoch: 99\t w: 2.01\t b: 0.95\t L: 0.0698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad3Q827dMohe"
      },
      "source": [
        "### Predictions\n",
        "\n",
        "Now that your model is trained, you can use it to predict some unknown test instances. Complete the code below to predict the output of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTqEAz6GMoRH",
        "outputId": "0f0e17f5-d999-4f18-d7c4-7829abd11ed7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_predictions = np.zeros((len(y_test),))\n",
        "for (i, x) in enumerate(x_test):\n",
        "    # TODO: Complete this\n",
        "    y_predictions[i] = model.forward(x)\n",
        "\n",
        "print(y_predictions)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 5.97796475  6.98350928 10.00014287]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2ANPLrkLVaY"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Finally, let us evaluate the linear regression model you developed. Unlike classification, we will need a different metric for regression. Recall from Lecture 3, a common evaluation metric for regression is the Mean Squared Error (MSE). We will use that for this tutorial.\n",
        "\n",
        "Complete the `mse()` function below to compute the MSE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fvD2vNXLx6f",
        "outputId": "72f578a2-0f4c-4bf5-abed-abd9617b7749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def mse(y_gold, y_prediction):\n",
        "    \"\"\" Compute the MSE given the ground truth and predictions\n",
        "\n",
        "    Args:\n",
        "        y_gold (np.ndarray): the correct ground truth values of y\n",
        "        y_prediction (np.ndarray): the predicted values of y\n",
        "\n",
        "    Returns:\n",
        "        float : MSE\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(y_gold) == len(y_prediction)\n",
        "\n",
        "    # TODO: Complete this\n",
        "    return np.square(y_gold - y_prediction).mean()\n",
        "\n",
        "\n",
        "# Compute the MSE on model predictions on our toy test data\n",
        "# You should be able to obtain a very small MSE rate\n",
        "print(mse(y_test, y_predictions))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.003576314517982279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6emtY9qqwGSe"
      },
      "source": [
        "## Iris Dataset (Extra exercise)\n",
        "\n",
        "Here is an extra optional exercise for you: try to get your simple linear regression model working on a (slightly) larger and noisier dataset.\n",
        "\n",
        "For this, we will convert the Iris dataset to use as a regression task. More specifically, our task is to predict the *petal width* of a flower (`y`) given the *sepal length* as input (`x`). The code below will give you the dataset in this format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1yTkyHzwnUp",
        "outputId": "eed877b3-c4ae-41be-9052-cefa4314ce8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Download iris data if it does not exist\n",
        "if not os.path.exists(\"iris.data\"):\n",
        "    !wget -O iris.data https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
        "\n",
        "def read_dataset_as_regression(filepath):\n",
        "    \"\"\" Read in the dataset from the specified filepath\n",
        "\n",
        "    Args:\n",
        "        filepath (str): The filepath to the dataset file\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x, y), each being a numpy array.\n",
        "               - x is a numpy array with shape (N, ),\n",
        "                   where N is the number of instances\n",
        "               - y is a numpy array with shape (N, ), where each element is a\n",
        "                real-valued float, and N is the number of instances\n",
        "    \"\"\"\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for line in open(filepath):\n",
        "        if line.strip() != \"\": # handle empty rows in file\n",
        "            row = line.strip().split(\",\")\n",
        "            # extract columns 0 as x.\n",
        "            x.append(float(row[0]))\n",
        "\n",
        "            # extract column 3 as y\n",
        "            y.append(float(row[3]))\n",
        "\n",
        "    return (np.array(x), np.array(y))\n",
        "\n",
        "\n",
        "def split_dataset(x, y, test_proportion, random_generator=default_rng()):\n",
        "    \"\"\" Split dataset into training and test sets, according to the given\n",
        "        test set proportion.\n",
        "\n",
        "    Args:\n",
        "        x (np.ndarray): Instances, numpy array with shape (N,K)\n",
        "        y (np.ndarray): Output label, numpy array with shape (N,)\n",
        "        test_proprotion (float): the desired proportion of test examples\n",
        "                                 (0.0-1.0)\n",
        "        random_generator (np.random.Generator): A random generator\n",
        "\n",
        "    Returns:\n",
        "        tuple: returns a tuple of (x_train, x_test, y_train, y_test)\n",
        "               - x_train (np.ndarray): Training instances shape (N_train, K)\n",
        "               - x_test (np.ndarray): Test instances shape (N_test, K)\n",
        "               - y_train (np.ndarray): Training labels, shape (N_train, )\n",
        "               - y_test (np.ndarray): Test labels, shape (N_test, )\n",
        "    \"\"\"\n",
        "\n",
        "    shuffled_indices = random_generator.permutation(len(x))\n",
        "    n_test = round(len(x) * test_proportion)\n",
        "    n_train = len(x) - n_test\n",
        "    x_train = x[shuffled_indices[:n_train]]\n",
        "    y_train = y[shuffled_indices[:n_train]]\n",
        "    x_test = x[shuffled_indices[n_train:]]\n",
        "    y_test = y[shuffled_indices[n_train:]]\n",
        "    return (x_train, x_test, y_train, y_test)\n",
        "\n",
        "\n",
        "(x, y) = read_dataset_as_regression(\"iris.data\")\n",
        "print(x.shape)  # (150,)\n",
        "print(y.shape)  # (150,)\n",
        "\n",
        "seed = 60012\n",
        "rg = default_rng(seed)\n",
        "x_train, x_test, y_train, y_test = split_dataset(x, y,\n",
        "                                                 test_proportion=0.2,\n",
        "                                                 random_generator=rg)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-20 14:25:49--  https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘iris.data’\n",
            "\n",
            "iris.data               [ <=>                ]   4.44K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-11-20 14:25:50 (59.4 MB/s) - ‘iris.data’ saved [4551]\n",
            "\n",
            "(150,)\n",
            "(150,)\n",
            "(120,)\n",
            "(30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XM2yXt_0RMO"
      },
      "source": [
        "As usual, it's always a good idea to examine your data before you start:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTF3kSAV0ddn",
        "outputId": "5ec77a73-708f-4c81-a7b1-9907183dac00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYm-2dB41JYI"
      },
      "source": [
        "### Model\n",
        "\n",
        "You can copy your `SimpleLinearRegression` class from earlier, with the`forward()`, `loss()` and `gradient()` methods that you implemented earlier. This time you can put them all together in the same place."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfdsAHaV1IuR"
      },
      "source": [
        "class SimpleLinearRegression:\n",
        "    def __init__(self, random_generator=default_rng()):\n",
        "        self.w = random_generator.standard_normal()\n",
        "        self.b = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w * x + self.b\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        y_hat = self.forward(x)\n",
        "        return (y_hat - y)**2\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        y_hat = self.forward(x)\n",
        "        return ((y_hat - y) * x, (y_hat - y))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6zXgIed3gTR"
      },
      "source": [
        "### Optimisation\n",
        "\n",
        "Again, you should not need to change much of your gradient descent implementation from earlier, so just copy it over. You may, however, need to tweak the learning rate and the number of epochs to obtain a reasonable output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Txe_vse3iIH",
        "outputId": "059af61e-87b4-4a70-cc2e-d8e504afde2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = SimpleLinearRegression()\n",
        "\n",
        "learning_rate = 0.0001\n",
        "n_epochs = 100\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    error = 0.0\n",
        "    grad_w = 0.0\n",
        "    grad_b = 0.0\n",
        "    for (x, y) in zip(x_train, y_train):\n",
        "        ### TODO: Complete this\n",
        "        ### 1. Compute the gradients for w and b for this example\n",
        "        (dLdw, dLdb) = model.gradient(x, y)\n",
        "\n",
        "        ### 2. Add the gradients to grad_w and grad_b\n",
        "        grad_w += dLdw\n",
        "        grad_b += dLdb\n",
        "\n",
        "        ### 3. Add the \"local\" loss to the global error (Loss) for analysis\n",
        "        error += model.loss(x, y)\n",
        "\n",
        "    # TODO: Update the weights using the (summed) gradients\n",
        "    model.w = model.w - learning_rate * grad_w\n",
        "    model.b = model.b - learning_rate * grad_b\n",
        "\n",
        "    print(f\"Epoch: {epoch}\\t w: {model.w:.2f}\\t b: {model.b:.2f}\\t L: {error:.4f}\")\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\t w: -0.34\t b: 0.07\t L: 3960.3818\n",
            "Epoch: 1\t w: -0.11\t b: 0.11\t L: 1313.9272\n",
            "Epoch: 2\t w: 0.02\t b: 0.13\t L: 457.9120\n",
            "Epoch: 3\t w: 0.09\t b: 0.14\t L: 181.0225\n",
            "Epoch: 4\t w: 0.13\t b: 0.14\t L: 91.4537\n",
            "Epoch: 5\t w: 0.16\t b: 0.15\t L: 62.4748\n",
            "Epoch: 6\t w: 0.17\t b: 0.15\t L: 53.0939\n",
            "Epoch: 7\t w: 0.18\t b: 0.15\t L: 50.0520\n",
            "Epoch: 8\t w: 0.19\t b: 0.15\t L: 49.0606\n",
            "Epoch: 9\t w: 0.19\t b: 0.15\t L: 48.7324\n",
            "Epoch: 10\t w: 0.19\t b: 0.15\t L: 48.6187\n",
            "Epoch: 11\t w: 0.19\t b: 0.15\t L: 48.5744\n",
            "Epoch: 12\t w: 0.19\t b: 0.15\t L: 48.5526\n",
            "Epoch: 13\t w: 0.19\t b: 0.15\t L: 48.5380\n",
            "Epoch: 14\t w: 0.19\t b: 0.15\t L: 48.5257\n",
            "Epoch: 15\t w: 0.19\t b: 0.15\t L: 48.5143\n",
            "Epoch: 16\t w: 0.19\t b: 0.14\t L: 48.5030\n",
            "Epoch: 17\t w: 0.19\t b: 0.14\t L: 48.4919\n",
            "Epoch: 18\t w: 0.19\t b: 0.14\t L: 48.4808\n",
            "Epoch: 19\t w: 0.19\t b: 0.14\t L: 48.4697\n",
            "Epoch: 20\t w: 0.19\t b: 0.14\t L: 48.4586\n",
            "Epoch: 21\t w: 0.19\t b: 0.14\t L: 48.4475\n",
            "Epoch: 22\t w: 0.19\t b: 0.14\t L: 48.4365\n",
            "Epoch: 23\t w: 0.19\t b: 0.14\t L: 48.4254\n",
            "Epoch: 24\t w: 0.19\t b: 0.14\t L: 48.4143\n",
            "Epoch: 25\t w: 0.19\t b: 0.14\t L: 48.4033\n",
            "Epoch: 26\t w: 0.19\t b: 0.14\t L: 48.3922\n",
            "Epoch: 27\t w: 0.19\t b: 0.14\t L: 48.3812\n",
            "Epoch: 28\t w: 0.19\t b: 0.14\t L: 48.3701\n",
            "Epoch: 29\t w: 0.19\t b: 0.14\t L: 48.3591\n",
            "Epoch: 30\t w: 0.19\t b: 0.13\t L: 48.3480\n",
            "Epoch: 31\t w: 0.19\t b: 0.13\t L: 48.3370\n",
            "Epoch: 32\t w: 0.19\t b: 0.13\t L: 48.3260\n",
            "Epoch: 33\t w: 0.19\t b: 0.13\t L: 48.3150\n",
            "Epoch: 34\t w: 0.19\t b: 0.13\t L: 48.3039\n",
            "Epoch: 35\t w: 0.19\t b: 0.13\t L: 48.2929\n",
            "Epoch: 36\t w: 0.19\t b: 0.13\t L: 48.2819\n",
            "Epoch: 37\t w: 0.19\t b: 0.13\t L: 48.2709\n",
            "Epoch: 38\t w: 0.19\t b: 0.13\t L: 48.2599\n",
            "Epoch: 39\t w: 0.19\t b: 0.13\t L: 48.2489\n",
            "Epoch: 40\t w: 0.19\t b: 0.13\t L: 48.2379\n",
            "Epoch: 41\t w: 0.20\t b: 0.13\t L: 48.2269\n",
            "Epoch: 42\t w: 0.20\t b: 0.13\t L: 48.2160\n",
            "Epoch: 43\t w: 0.20\t b: 0.13\t L: 48.2050\n",
            "Epoch: 44\t w: 0.20\t b: 0.12\t L: 48.1940\n",
            "Epoch: 45\t w: 0.20\t b: 0.12\t L: 48.1831\n",
            "Epoch: 46\t w: 0.20\t b: 0.12\t L: 48.1721\n",
            "Epoch: 47\t w: 0.20\t b: 0.12\t L: 48.1611\n",
            "Epoch: 48\t w: 0.20\t b: 0.12\t L: 48.1502\n",
            "Epoch: 49\t w: 0.20\t b: 0.12\t L: 48.1392\n",
            "Epoch: 50\t w: 0.20\t b: 0.12\t L: 48.1283\n",
            "Epoch: 51\t w: 0.20\t b: 0.12\t L: 48.1173\n",
            "Epoch: 52\t w: 0.20\t b: 0.12\t L: 48.1064\n",
            "Epoch: 53\t w: 0.20\t b: 0.12\t L: 48.0955\n",
            "Epoch: 54\t w: 0.20\t b: 0.12\t L: 48.0846\n",
            "Epoch: 55\t w: 0.20\t b: 0.12\t L: 48.0736\n",
            "Epoch: 56\t w: 0.20\t b: 0.12\t L: 48.0627\n",
            "Epoch: 57\t w: 0.20\t b: 0.11\t L: 48.0518\n",
            "Epoch: 58\t w: 0.20\t b: 0.11\t L: 48.0409\n",
            "Epoch: 59\t w: 0.20\t b: 0.11\t L: 48.0300\n",
            "Epoch: 60\t w: 0.20\t b: 0.11\t L: 48.0191\n",
            "Epoch: 61\t w: 0.20\t b: 0.11\t L: 48.0082\n",
            "Epoch: 62\t w: 0.20\t b: 0.11\t L: 47.9973\n",
            "Epoch: 63\t w: 0.20\t b: 0.11\t L: 47.9864\n",
            "Epoch: 64\t w: 0.20\t b: 0.11\t L: 47.9756\n",
            "Epoch: 65\t w: 0.20\t b: 0.11\t L: 47.9647\n",
            "Epoch: 66\t w: 0.20\t b: 0.11\t L: 47.9538\n",
            "Epoch: 67\t w: 0.20\t b: 0.11\t L: 47.9430\n",
            "Epoch: 68\t w: 0.20\t b: 0.11\t L: 47.9321\n",
            "Epoch: 69\t w: 0.20\t b: 0.11\t L: 47.9212\n",
            "Epoch: 70\t w: 0.20\t b: 0.11\t L: 47.9104\n",
            "Epoch: 71\t w: 0.20\t b: 0.10\t L: 47.8995\n",
            "Epoch: 72\t w: 0.20\t b: 0.10\t L: 47.8887\n",
            "Epoch: 73\t w: 0.20\t b: 0.10\t L: 47.8779\n",
            "Epoch: 74\t w: 0.20\t b: 0.10\t L: 47.8670\n",
            "Epoch: 75\t w: 0.20\t b: 0.10\t L: 47.8562\n",
            "Epoch: 76\t w: 0.20\t b: 0.10\t L: 47.8454\n",
            "Epoch: 77\t w: 0.20\t b: 0.10\t L: 47.8346\n",
            "Epoch: 78\t w: 0.20\t b: 0.10\t L: 47.8238\n",
            "Epoch: 79\t w: 0.20\t b: 0.10\t L: 47.8129\n",
            "Epoch: 80\t w: 0.20\t b: 0.10\t L: 47.8021\n",
            "Epoch: 81\t w: 0.20\t b: 0.10\t L: 47.7913\n",
            "Epoch: 82\t w: 0.20\t b: 0.10\t L: 47.7805\n",
            "Epoch: 83\t w: 0.20\t b: 0.10\t L: 47.7698\n",
            "Epoch: 84\t w: 0.20\t b: 0.10\t L: 47.7590\n",
            "Epoch: 85\t w: 0.20\t b: 0.09\t L: 47.7482\n",
            "Epoch: 86\t w: 0.20\t b: 0.09\t L: 47.7374\n",
            "Epoch: 87\t w: 0.20\t b: 0.09\t L: 47.7266\n",
            "Epoch: 88\t w: 0.20\t b: 0.09\t L: 47.7159\n",
            "Epoch: 89\t w: 0.20\t b: 0.09\t L: 47.7051\n",
            "Epoch: 90\t w: 0.20\t b: 0.09\t L: 47.6943\n",
            "Epoch: 91\t w: 0.20\t b: 0.09\t L: 47.6836\n",
            "Epoch: 92\t w: 0.20\t b: 0.09\t L: 47.6728\n",
            "Epoch: 93\t w: 0.20\t b: 0.09\t L: 47.6621\n",
            "Epoch: 94\t w: 0.20\t b: 0.09\t L: 47.6514\n",
            "Epoch: 95\t w: 0.20\t b: 0.09\t L: 47.6406\n",
            "Epoch: 96\t w: 0.20\t b: 0.09\t L: 47.6299\n",
            "Epoch: 97\t w: 0.20\t b: 0.09\t L: 47.6192\n",
            "Epoch: 98\t w: 0.20\t b: 0.09\t L: 47.6084\n",
            "Epoch: 99\t w: 0.20\t b: 0.08\t L: 47.5977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P7tQX-L31LB"
      },
      "source": [
        "### Visualising your trained model\n",
        "\n",
        "You can visualise your model by plotting the line on the graph.\n",
        "\n",
        "We will also plot the test instances to get a rough idea of how well we expect it to perform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUmFEg7s33Ve",
        "outputId": "70780865-0471-4a19-9bbd-68e89e6d8df0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "# Plot training instances\n",
        "plt.figure()\n",
        "plt.scatter(x_train, y_train, c=\"blue\", edgecolor='k')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "\n",
        "# Draw the line representing the model\n",
        "xmin = x_train.min()\n",
        "ymin = model.forward(xmin)\n",
        "xmax = x_train.max()\n",
        "ymax = model.forward(xmax)\n",
        "plt.plot([xmin, xmax], [ymin, ymax], 'r-')\n",
        "\n",
        "# Plot test instances\n",
        "plt.scatter(x_test, y_test, c=\"red\", edgecolor='k')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY37EHKe5JZT"
      },
      "source": [
        "### Predictions and evaluation\n",
        "\n",
        "Finally, predict the test instances given the model.\n",
        "\n",
        "Then evaluate the model with MSE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHx0gaAr5OfN",
        "outputId": "245cdb24-b0fd-400c-90da-ef12edd06af5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_predictions = np.zeros((len(y_test),))\n",
        "for (i, x) in enumerate(x_test):\n",
        "    y_predictions[i] = model.forward(x)\n",
        "\n",
        "print(y_predictions)\n",
        "print(y_test)\n",
        "\n",
        "print(mse(y_test, y_predictions))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.41836104 1.35773277 1.2162668  1.68108356 1.11521968 1.19605737\n",
            " 1.58003644 1.11521968 1.0545914  1.39815162 1.45877989 1.25668565\n",
            " 1.33752335 1.1354291  1.33752335 1.07480083 0.97375371 1.47898932\n",
            " 1.31731392 1.43857047 1.2162668  1.31731392 1.31731392 1.09501025\n",
            " 1.49919874 0.95354428 1.0545914  1.17584795 1.03438198 0.99396313]\n",
            "[1.4 1.8 1.1 2.  0.2 1.1 1.9 0.4 0.1 1.8 2.3 2.4 1.8 0.2 2.3 0.1 0.2 2.3\n",
            " 1.4 1.7 2.  1.2 1.4 1.  1.4 0.1 0.3 0.4 0.2 0.3]\n",
            "0.4301431203882206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNMajcyH7Vv5"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Hopefully you have managed to deepen your understanding about linear regression by implementing the model, loss function, and the gradient descent algorithm, and putting everything together for training and testing.\n",
        "\n",
        "In the next lab exercise, we will delve a bit deeper at implementation level, and try to extend your model to handle more than one input variable. We will also start making your code a bit more efficient with vectorised implementations so that you can perform computations on multiple training instances simultaneously. This will hopefully help you get started on implementing Neural Networks (which we will unfortunately not cover in these lab exercises as it is part of your second coursework).  \n",
        "\n"
      ]
    }
  ]
}